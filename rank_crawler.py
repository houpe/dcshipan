#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""


åŠŸèƒ½ï¼š
1. è·å–5ä¸ªæ’è¡Œæ¦œæ•°æ®ï¼ˆæ—¥æ¦œã€å‘¨æ¦œã€æœˆæ¦œã€å¹´æ¦œã€æ€»æ¦œï¼‰
2. ä½¿ç”¨DiskCacheç¼“å­˜æ•°æ®
3. æ·»åŠ æ’åä¿¡æ¯
4. å®šæ—¶æ‰§è¡Œï¼ˆäº¤æ˜“æ—¶é—´å†…æ¯30ç§’æ‰§è¡Œä¸€æ¬¡ï¼‰
5. æ˜¾ç¤ºè¿›åº¦æ¡å’Œç»Ÿè®¡ä¿¡æ¯
"""

import asyncio
import aiohttp
import time
import logging
import os
import json
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from tqdm import tqdm
import diskcache as dc
import schedule
import threading

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RankCrawler:
    def __init__(self, cache_dir: str = "data/cache", enable_deduplication: bool = True):
        """åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
            enable_deduplication: æ˜¯å¦å¯ç”¨å»é‡åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
        """
        self.base_url = "https://emdcspzhapi.dfcfs.cn/rtV1"
        self.cache_dir = cache_dir
        self.enable_deduplication = enable_deduplication
        self.ensure_cache_dir()
        
        # åˆå§‹åŒ–DiskCache
        self.cache = dc.Cache(os.path.join(self.cache_dir, 'rank_cache'))
        
        # æ’è¡Œæ¦œç±»å‹é…ç½®
        count = 100
        self.rank_types = {
            "daily": {"rankType": "10005", "name": "æ—¥æ¦œ", "recCnt": count},
            "weekly": {"rankType": "10000", "name": "å‘¨æ¦œ", "recCnt": count},
            "monthly": {"rankType": "10001", "name": "æœˆæ¦œ", "recCnt": count},
            "yearly": {"rankType": "10003", "name": "å¹´æ¦œ", "recCnt": count},
            "total": {"rankType": "10004", "name": "æ€»æ¦œ", "recCnt": count}
        }
        
        self.session = None
        self.running = False
        
        logger.info(f"çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œå»é‡åŠŸèƒ½: {'å¯ç”¨' if self.enable_deduplication else 'ç¦ç”¨'}")
        
    def ensure_cache_dir(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    async def create_session(self):
        """åˆ›å»ºHTTPä¼šè¯"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            connector = aiohttp.TCPConnector(limit=100, limit_per_host=30)
            self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
    
    async def close_session(self):
        """å…³é—­HTTPä¼šè¯"""
        if self.session and not self.session.closed:
            await self.session.close()
            self.session = None
    
    async def fetch_with_retry(self, url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """å¸¦é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚"""
        await self.create_session()
            
        for attempt in range(max_retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        text = await response.text()
                        try:
                            data = json.loads(text)
                            if isinstance(data, dict) and data.get('result') == '0':
                                return data
                            else:
                                logger.warning(f"APIè¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
                        except json.JSONDecodeError:
                            logger.warning(f"æ— æ³•è§£æJSONå“åº”: {text[:100]}...")
                    else:
                        logger.warning(f"HTTPè¯·æ±‚å¤±è´¥: {response.status}")
            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
        
        return None
    
    async def fetch_rank_data(self, rank_type: str, rank_config: Dict) -> Optional[Dict[str, Any]]:
        """è·å–å•ä¸ªæ’è¡Œæ¦œæ•°æ®ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
        all_data = []
        target_count = rank_config['recCnt']
        page_size = 20  # APIé™åˆ¶æ¯æ¬¡æœ€å¤š20æ¡
        current_page = 0
        
        while len(all_data) < target_count:
            # è®¡ç®—æœ¬æ¬¡è¯·æ±‚çš„æ•°é‡
            remaining = target_count - len(all_data)
            current_count = min(page_size, remaining)
            
            url = f"{self.base_url}?type=rt_get_rank&rankType={rank_config['rankType']}&recIdx={current_page}&recCnt={current_count}&rankid=0"
            
            data = await self.fetch_with_retry(url)
            if data and 'data' in data and len(data['data']) > 0:
                all_data.extend(data['data'])
                current_page += 1
                
                # å¦‚æœè¿”å›çš„æ•°æ®å°‘äºè¯·æ±‚çš„æ•°é‡ï¼Œè¯´æ˜å·²ç»åˆ°åº•äº†
                if len(data['data']) < current_count:
                    break
            else:
                # å¦‚æœè¯·æ±‚å¤±è´¥æˆ–æ²¡æœ‰æ•°æ®ï¼Œåœæ­¢åˆ†é¡µ
                break
        
        if all_data:
            # æ·»åŠ æ’åä¿¡æ¯
            for i, item in enumerate(all_data, 1):
                item['rank'] = i
                item['rank_type'] = rank_type
                item['rank_name'] = rank_config['name']
            
            # æ„é€ è¿”å›æ•°æ®æ ¼å¼
            result_data = {
                'result': '0',
                'data': all_data[:target_count]  # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
            }
            
            return result_data
    
    def _parse_combo_html(self, combo_html):
        """è§£æç»„åˆHTMLå­—ç¬¦ä¸²ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯"""
        import re
        
        # æå–ç»„åˆåç§°å’ŒID
        combo_match = re.search(r'data-combo-id="([^"]+)"[^>]*data-combo-name="([^"]+)"[^>]*>([^<]+)', combo_html)
        if not combo_match:
            return None
        
        combo_id = combo_match.group(1)
        combo_name = combo_match.group(2)
        display_name = combo_match.group(3)
        
        # æå–æ’åæ ‡ç­¾
        ranks = []
        rank_matches = re.findall(r'<span class="rank-tag"[^>]*data-rank-type="([^"]+)"[^>]*>([^<]+)</span>', combo_html)
        for rank_type, rank_text in rank_matches:
            ranks.append({
                'type': rank_type,
                'text': rank_text
            })
        
        return {
            'id': combo_id,
            'name': combo_name,
            'display_name': display_name,
            'ranks': ranks
        }
    
    async def fetch_all_rank_data(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®"""
        await self.create_session()
        
        results = {}
        total_tasks = len(self.rank_types)
        
        with tqdm(total=total_tasks, desc="è·å–æ’è¡Œæ¦œæ•°æ®", unit="ä¸ª") as pbar:
            start_time = time.time()
            
            # å¹¶å‘è·å–æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®
            tasks = []
            for rank_type, rank_config in self.rank_types.items():
                task = self.fetch_rank_data(rank_type, rank_config)
                tasks.append((rank_type, task))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for rank_type, task in tasks:
                data = await task
                if data:
                    results[rank_type] = data
                
                pbar.update(1)
                elapsed_time = time.time() - start_time
                pbar.set_postfix({
                    'å·²å®Œæˆ': f"{pbar.n}/{total_tasks}",
                    'è€—æ—¶': f"{elapsed_time:.2f}s"
                })
        
        await self.close_session()
        return results
    
    def deduplicate_rank_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ’è¡Œæ¦œæ•°æ®è¿›è¡Œå»é‡ï¼Œä¼˜å…ˆçº§ï¼šæ€»æ¦œ>å¹´æ¦œ>æœˆæ¦œ>å‘¨æ¦œ>æ—¥æ¦œ"""
        # å®šä¹‰æ¦œå•ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        rank_priority = {
            'total': 1,   # æ€»æ¦œ
            'yearly': 2,  # å¹´æ¦œ
            'monthly': 3, # æœˆæ¦œ
            'weekly': 4,  # å‘¨æ¦œ
            'daily': 5    # æ—¥æ¦œ
        }
        
        # å­˜å‚¨æ‰€æœ‰ç»„åˆæ•°æ®ï¼Œkeyä¸ºç»„åˆIDï¼Œvalueä¸º(ä¼˜å…ˆçº§, æ¦œå•ç±»å‹, æ•°æ®)
        portfolio_map = {}
        
        # éå†æ‰€æœ‰æ¦œå•æ•°æ®
        for rank_type, rank_data in data.items():
            priority = rank_priority.get(rank_type, 999)  # æœªçŸ¥ç±»å‹è®¾ä¸ºæœ€ä½ä¼˜å…ˆçº§
            
            for item in rank_data.get('data', []):
                portfolio_id = item.get('zjzh')  # ç»„åˆID
                if portfolio_id:
                    # å¦‚æœè¯¥ç»„åˆè¿˜æœªè®°å½•ï¼Œæˆ–å½“å‰æ¦œå•ä¼˜å…ˆçº§æ›´é«˜ï¼Œåˆ™æ›´æ–°è®°å½•
                    if (portfolio_id not in portfolio_map or 
                        priority < portfolio_map[portfolio_id][0]):
                        portfolio_map[portfolio_id] = (priority, rank_type, item)
        
        # é‡æ–°æ„å»ºå»é‡åçš„æ•°æ®ç»“æ„
        deduplicated_data = {}
        for rank_type in data.keys():
            deduplicated_data[rank_type] = {
                'result': '0',
                'data': []
            }
        
        # å°†å»é‡åçš„æ•°æ®åˆ†é…å›å¯¹åº”çš„æ¦œå•
        for portfolio_id, (priority, rank_type, item) in portfolio_map.items():
            deduplicated_data[rank_type]['data'].append(item)
        
        # é‡æ–°æ’åºæ¯ä¸ªæ¦œå•çš„æ•°æ®ï¼ˆæŒ‰åŸæœ‰æ’åï¼‰
        for rank_type, rank_data in deduplicated_data.items():
            rank_data['data'].sort(key=lambda x: x.get('rank', 999))
            # é‡æ–°åˆ†é…æ’å
            for i, item in enumerate(rank_data['data'], 1):
                item['rank'] = i
        
        return deduplicated_data
    
    def save_rank_data(self, data: Dict[str, Any]) -> str:
        """ä¿å­˜æ’è¡Œæ¦œæ•°æ®åˆ°DiskCache"""
        date_str = datetime.now().strftime("%Y%m%d")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¿›è¡Œå»é‡å¤„ç†
        if self.enable_deduplication:
            deduplicated_data = self.deduplicate_rank_data(data)
        else:
            deduplicated_data = data
        
        # ç»Ÿè®¡æ€»è®°å½•æ•°
        total_records = sum(len(rank_data.get('data', [])) for rank_data in deduplicated_data.values())
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        merged_data = {
            "timestamp": timestamp,
            "date": datetime.now().strftime("%Y-%m-%d"),
            "update_time": datetime.now().isoformat(),
            "total_records": total_records,
            "data": deduplicated_data
        }
        
        # ä½¿ç”¨æ—¥æœŸä½œä¸ºç¼“å­˜é”®
        cache_key = f"rank_data_{date_str}"
        
        # æ¸…é™¤å½“å¤©çš„æ—§ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if cache_key in self.cache:
            del self.cache[cache_key]
            logger.info(f"ğŸ—‘ï¸ æ¸…é™¤æ—§ç¼“å­˜: {cache_key}")
        
        # ä¿å­˜åˆ°DiskCache
        self.cache.set(cache_key, merged_data)
        
        # ä¿å­˜æœ€æ–°æ•°æ®
        self.cache.set("latest_rank_data", merged_data)
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self.clean_expired_cache(CACHE_EXPIRE_HOURS)
        
        logger.info(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜: {cache_key}, è®°å½•æ•°: {total_records}æ¡")
        return cache_key
    
    def load_latest_rank_data(self) -> Optional[Dict[str, Any]]:
        """ä»DiskCacheåŠ è½½æœ€æ–°çš„æ’è¡Œæ¦œæ•°æ®"""
        try:
            data = self.cache.get("latest_rank_data")
            if data is not None:
                return data
        except Exception as e:
            logger.error(f"ä»DiskCacheåŠ è½½æ’è¡Œæ¦œæ•°æ®å¤±è´¥: {str(e)}")
        
        return None
    
    def clean_expired_cache(self, expire_hours: int = 1):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ•°æ®"""
        try:
            current_time = datetime.now()
            expire_threshold = current_time - timedelta(hours=expire_hours)
            
            # è·å–æ‰€æœ‰ä»¥rank_data_å¼€å¤´çš„ç¼“å­˜é”®
            expired_keys = []
            for key in self.cache:
                if isinstance(key, str) and key.startswith('rank_data_'):
                    try:
                        # ä»é”®åä¸­æå–æ—¥æœŸ
                        date_str = key.replace('rank_data_', '')
                        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šYYYYMMDD å’Œ YYYYMMDD_HHMMSS
                        if len(date_str) == 8:  # æ–°æ ¼å¼ï¼šYYYYMMDD
                            cache_time = datetime.strptime(date_str, '%Y%m%d')
                        elif len(date_str) == 15:  # æ—§æ ¼å¼ï¼šYYYYMMDD_HHMMSS
                            cache_time = datetime.strptime(date_str, '%Y%m%d_%H%M%S')
                        else:
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                        if cache_time < expire_threshold:
                            expired_keys.append(key)
                    except ValueError:
                        # å¦‚æœæ—¶é—´æˆ³æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡
                        continue
            
            # åˆ é™¤è¿‡æœŸçš„ç¼“å­˜
            if expired_keys:
                for key in expired_keys:
                    del self.cache[key]
                logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)}ä¸ªé¡¹ç›®")
                
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {str(e)}")
    
    def is_trading_time(self) -> bool:
        """æ£€æŸ¥å½“å‰æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´"""
        now = datetime.now()
        current_time = now.time()
        
        # äº¤æ˜“æ—¶é—´ï¼š9:15-11:30, 13:00-15:00
        morning_start = datetime.strptime('09:15', '%H:%M').time()
        morning_end = datetime.strptime('11:30', '%H:%M').time()
        afternoon_start = datetime.strptime('13:00', '%H:%M').time()
        afternoon_end = datetime.strptime('15:00', '%H:%M').time()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå·¥ä½œæ—¥ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰
        if now.weekday() >= 5:  # å‘¨å…­ã€å‘¨æ—¥
            return False
            
        # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…
        is_morning = morning_start <= current_time <= morning_end
        is_afternoon = afternoon_start <= current_time <= afternoon_end
        
        return is_morning or is_afternoon
    
    async def run_update(self) -> bool:
        """æ‰§è¡Œä¸€æ¬¡æ•°æ®æ›´æ–°"""
        try:
            logger.info("ğŸŒ å¼€å§‹å®æ—¶è¯·æ±‚æ’è¡Œæ¦œæ•°æ®...")
            start_time = time.time()
            
            # è·å–æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®
            rank_data = await self.fetch_all_rank_data()
            
            if rank_data:
                # ä¿å­˜æ•°æ®
                cache_key = self.save_rank_data(rank_data)
                
                elapsed_time = time.time() - start_time
                total_records = sum(len(data.get('data', [])) for data in rank_data.values())
                
                logger.info(f"ğŸŒ å®æ—¶æ•°æ®è·å–å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’, æ¦œå•: {len(rank_data)}ä¸ª, è®°å½•: {total_records}æ¡")
                
                return True
            else:
                logger.error("æœªè·å–åˆ°ä»»ä½•æ’è¡Œæ¦œæ•°æ®")
                return False
                
        except Exception as e:
            logger.error(f"æ•°æ®æ›´æ–°å¤±è´¥: {str(e)}")
            return False
    
    def update_job(self):
        """å®šæ—¶æ›´æ–°ä»»åŠ¡"""
        if ENABLE_TRADING_TIME_CHECK and not self.is_trading_time():
            logger.info("å½“å‰éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡æ•°æ®æ›´æ–°")
            return
            
        logger.info("å¼€å§‹å®šæ—¶æ•°æ®æ›´æ–°...")
        
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        def run_async_update():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                success = loop.run_until_complete(self.run_update())
                if success:
                    logger.info("å®šæ—¶æ•°æ®æ›´æ–°æˆåŠŸ")
                else:
                    logger.warning("å®šæ—¶æ•°æ®æ›´æ–°å¤±è´¥")
            except Exception as e:
                logger.error(f"å®šæ—¶æ•°æ®æ›´æ–°å¼‚å¸¸: {str(e)}")
            finally:
                loop.close()
        
        # åœ¨çº¿ç¨‹ä¸­è¿è¡Œä»¥é¿å…é˜»å¡
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            try:
                future = executor.submit(run_async_update)
                future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
            except concurrent.futures.TimeoutError:
                logger.error("å®šæ—¶æ›´æ–°ä»»åŠ¡è¶…æ—¶")
            except Exception as e:
                logger.error(f"å®šæ—¶æ›´æ–°ä»»åŠ¡å¤±è´¥: {str(e)}")
    
    def setup_schedule(self, refresh_interval=30):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        # äº¤æ˜“æ—¶é—´å†…æŒ‰æŒ‡å®šé—´éš”æ‰§è¡Œ
        schedule.every(refresh_interval).seconds.do(self.update_job)
        
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼šäº¤æ˜“æ—¶é—´å†…æ¯{refresh_interval}ç§’æ›´æ–°ä¸€æ¬¡æ•°æ®")
        logger.info("äº¤æ˜“æ—¶é—´ï¼šå‘¨ä¸€è‡³å‘¨äº” 9:15-11:30, 13:00-15:00")
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
        self.running = True
        
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")
        
        while self.running:
            schedule.run_pending()
            time.sleep(1)
    
    def stop_scheduler(self):
        """åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
        self.running = False
        schedule.clear()
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")
    
    def start_scheduler_in_thread(self):
        """åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
        def run_scheduler():
            self.start_scheduler()
        
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨")
        return scheduler_thread
    
    def print_latest_data_summary(self):
        """æ‰“å°æœ€æ–°æ•°æ®æ‘˜è¦"""
        data = self.load_latest_rank_data()
        if data:
            print(f"\n=== æœ€æ–°æ’è¡Œæ¦œæ•°æ®æ‘˜è¦ ===")
            print(f"æ›´æ–°æ—¶é—´: {data.get('update_time', 'Unknown')}")
            print(f"æ€»è®°å½•æ•°: {data.get('total_records', 0)}æ¡")
            
            for rank_type, rank_data in data.get('data', {}).items():
                rank_name = self.rank_types.get(rank_type, {}).get('name', rank_type)
                record_count = len(rank_data.get('data', []))
                print(f"  {rank_name}: {record_count}æ¡")
                
                # æ˜¾ç¤ºå‰3å
                if rank_data.get('data'):
                    print(f"    å‰3å:")
                    for i, item in enumerate(rank_data['data'][:3], 1):
                        name = item.get('zhuheName', 'Unknown')
                        rate = item.get('rateForApp', 'N/A')
                        rate_title = item.get('rateTitle', '')
                        print(f"      {i}. {name} ({rate_title}: {rate}%)")
            print("=" * 40)
        else:
            print("æš‚æ— ç¼“å­˜æ•°æ®")
    
    async def get_cached_rank_list_with_auto_update(self) -> Dict[str, Any]:
        """è¯»å–ç¼“å­˜ä¸­çš„ç»„åˆåˆ—è¡¨ï¼Œè‹¥è·å–çš„æ•°æ®ä¸º0ï¼Œå°è¯•é‡æ–°æ›´æ–°ç»„åˆåˆ—è¡¨"""
        import time
        start_time = time.time()
        
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½æ•°æ®
        cached_data = self.load_latest_rank_data()
        
        if cached_data:
            total_records = cached_data.get('total_records', 0)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if total_records > 0:
                elapsed_time = time.time() - start_time
                print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ•°æ® - è€—æ—¶: {elapsed_time:.2f}ç§’, æ•°æ®: {total_records}æ¡")
                return cached_data
        
        # å¦‚æœç¼“å­˜ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è·å–æ•°æ®
        try:
            print("ğŸ”„ ç¼“å­˜æ— æ•ˆï¼Œå¼€å§‹å®æ—¶è¯·æ±‚æ’è¡Œæ¦œæ•°æ®...")
            update_start = time.time()
            success = await self.run_update()
            
            if success:
                # é‡æ–°åŠ è½½æ›´æ–°åçš„æ•°æ®
                updated_data = self.load_latest_rank_data()
                if updated_data:
                    total_records = updated_data.get('total_records', 0)
                    elapsed_time = time.time() - update_start
                    print(f"âœ… å®æ—¶æ•°æ®è·å–å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’, æ•°æ®: {total_records}æ¡")
                    return updated_data
                else:
                    print("âŒ å®æ—¶è¯·æ±‚åä»æ— æ³•è·å–æ•°æ®")
                    return {}
            else:
                print("âŒ å®æ—¶æ•°æ®è·å–å¤±è´¥")
                return {}
                
        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"âŒ å®æ—¶æ•°æ®è·å–å¼‚å¸¸ - è€—æ—¶: {elapsed_time:.2f}ç§’, é”™è¯¯: {str(e)}")
            logger.error(f"é‡æ–°è·å–æ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    async def fetch_portfolio_holdings(self, portfolio_id: str, rec_count: int = 50) -> Optional[Dict[str, Any]]:
        """è·å–å•ä¸ªç»„åˆçš„è°ƒä»“è®°å½•"""
        url = f"https://emdcspzhapi.dfcfs.cn/rtV1?type=rt_hold_change72&zh={portfolio_id}&recIdx=1&recCnt={rec_count}"
        
        try:
            if not self.session:
                await self.create_session()
            
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('result') == '0':
                        return data
                    else:
                        logger.warning(f"è·å–ç»„åˆ{portfolio_id}è°ƒä»“è®°å½•å¤±è´¥: {data.get('message', 'Unknown error')}")
                        return None
                else:
                    logger.error(f"è·å–ç»„åˆ{portfolio_id}è°ƒä»“è®°å½•HTTPé”™è¯¯: {response.status}")
                    return None
                    
        except Exception as e:
            logger.error(f"è·å–ç»„åˆ{portfolio_id}è°ƒä»“è®°å½•å¼‚å¸¸: {str(e)}")
            return None
    
    def parse_trading_action(self, cwhj_mr: str, cwhj_mc: str) -> Optional[str]:
        """è§£æäº¤æ˜“åŠ¨ä½œï¼ˆä¹°å…¥/å–å‡ºï¼‰"""
        # cwhj_mr è‹¥ä¸æ˜¯-ï¼Œåˆ™ä»£è¡¨ä¹°å…¥ï¼ˆåŒ…å«"æˆ"çš„æ¯”ä¾‹ä¹Ÿæ˜¯æœ‰æ•ˆçš„ä¹°å…¥æ“ä½œï¼‰
        if cwhj_mr and cwhj_mr != "-":
            return "ä¹°å…¥"
        
        # cwhj_mc è‹¥ä¸æ˜¯-ï¼Œåˆ™ä»£è¡¨å–å‡ºï¼ˆåŒ…å«"æˆ"çš„æ¯”ä¾‹ä¹Ÿæ˜¯æœ‰æ•ˆçš„å–å‡ºæ“ä½œï¼‰
        if cwhj_mc and cwhj_mc != "-":
            return "å–å‡º"
        
        return None
    
    def is_a_stock_trading_day(self, date: datetime) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAè‚¡äº¤æ˜“æ—¥
        
        Aè‚¡äº¤æ˜“æ—¥è§„åˆ™ï¼š
        - å‘¨ä¸€åˆ°å‘¨äº”
        - æ’é™¤æ³•å®šèŠ‚å‡æ—¥ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªæ’é™¤å‘¨æœ«ï¼‰
        """
        # å‘¨ä¸€=0, å‘¨æ—¥=6
        weekday = date.weekday()
        # Aè‚¡äº¤æ˜“æ—¥ï¼šå‘¨ä¸€åˆ°å‘¨äº”ï¼ˆ0-4ï¼‰
        return weekday < 5
    
    def get_last_trading_day(self, from_date: datetime) -> datetime:
        """è·å–æŒ‡å®šæ—¥æœŸä¹‹å‰çš„æœ€è¿‘ä¸€ä¸ªAè‚¡äº¤æ˜“æ—¥"""
        current_date = from_date
        while True:
            current_date = current_date - timedelta(days=1)
            if self.is_a_stock_trading_day(current_date):
                return current_date
    
    def get_target_date(self, days_back: int = 0) -> str:
        """è·å–ç›®æ ‡æ—¥æœŸï¼ˆæ ¹æ®Aè‚¡äº¤æ˜“æ—¥è§„åˆ™ï¼‰
        
        è§„åˆ™ï¼š
        - 0ç‚¹åˆ°8ç‚¹30åˆ†ï¼šè°ƒä»“æ—¥æœŸæ˜¯å‰ä¸€ä¸ªAè‚¡äº¤æ˜“æ—¥
        - 8ç‚¹30åˆ†åˆ°24ç‚¹ï¼šè°ƒä»“æ—¥æœŸæ˜¯å½“å¤©ï¼Œè‹¥å½“å¤©ä¸æ˜¯Aè‚¡äº¤æ˜“æ—¥ï¼Œåˆ™å–ä¸Šä¸€ä¸ªæœ€è¿‘çš„äº¤æ˜“æ—¥
        """
        now = datetime.now()
        current_time = now.time()
        
        # åº”ç”¨days_backåç§»
        base_date = now - timedelta(days=days_back)
        
        # æ ¹æ®æ—¶é—´è§„åˆ™ç¡®å®šç›®æ ‡æ—¥æœŸ
        if current_time < datetime.strptime('08:30', '%H:%M').time():
            # 0ç‚¹åˆ°8ç‚¹30åˆ†ï¼šå–å‰ä¸€ä¸ªAè‚¡äº¤æ˜“æ—¥
            target_date = self.get_last_trading_day(base_date)
        else:
            # 8ç‚¹30åˆ†åˆ°24ç‚¹ï¼šå–å½“å¤©ï¼Œè‹¥ä¸æ˜¯äº¤æ˜“æ—¥åˆ™å–ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
            if self.is_a_stock_trading_day(base_date):
                target_date = base_date
            else:
                target_date = self.get_last_trading_day(base_date)
        
        return target_date.strftime('%Y%m%d')
    
    async def tc_list(self, days_back: int = 0, max_days_search: int = 3, use_cache: bool = True) -> List[Dict[str, Any]]:
        """è·å–ç»„åˆè°ƒä»“è®°å½•åˆ—è¡¨
        
        Args:
            days_back: å¾€å‰æ¨å‡ å¤©å¼€å§‹æŸ¥æ‰¾ï¼ˆ0è¡¨ç¤ºå½“å¤©æˆ–å‰ä¸€å¤©ï¼‰
            max_days_search: æœ€å¤šæœç´¢å‡ å¤©çš„æ•°æ®
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼ˆFalse=å®æ—¶è¯·æ±‚ï¼‰
        """
        import time
        start_time = time.time()
        
        # 1. è·å–ç»„åˆåˆ—è¡¨æ•°æ®
        if use_cache:
            print("ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ•°æ®è·å–ç»„åˆåˆ—è¡¨")
            cached_data = await self.get_cached_rank_list_with_auto_update()
        else:
            print("ğŸŒ å®æ—¶è¯·æ±‚ç»„åˆåˆ—è¡¨æ•°æ®")
            cached_data = await self.fetch_all_rank_data()
            
            # å¦‚æœå®æ—¶è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜æ•°æ®
            if not cached_data or not cached_data.get('data'):
                print("âš ï¸ å®æ—¶è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜æ•°æ®")
                cached_data = await self.get_cached_rank_list_with_auto_update()
        
        if not cached_data or not cached_data.get('data'):
            print("âŒ æ— æ³•è·å–ç»„åˆåˆ—è¡¨æ•°æ®")
            return []
        
        # 2. è·å–ç›®æ ‡æ—¥æœŸåˆ—è¡¨
        target_dates = []
        for i in range(max_days_search):
            date = self.get_target_date(days_back + i)
            target_dates.append(date)
        
        # 3. æ”¶é›†æ‰€æœ‰ç»„åˆIDå’Œåç§°
        portfolios = []
        for rank_type, rank_data in cached_data.get('data', {}).items():
            for item in rank_data.get('data', []):
                portfolio_id = item.get('zjzh')
                portfolio_name = item.get('zhuheName')
                if portfolio_id and portfolio_name:
                    portfolios.append({
                        'id': portfolio_id,
                        'name': portfolio_name
                    })
        
        # 4. å¼‚æ­¥è·å–æ‰€æœ‰ç»„åˆçš„è°ƒä»“è®°å½•
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = []
        for portfolio in portfolios:
            task = self.fetch_portfolio_holdings(portfolio['id'])
            tasks.append((portfolio, task))
        
        # æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
        results = []
        with tqdm(total=len(tasks), desc="è·å–è°ƒä»“è®°å½•", unit="ä¸ª") as pbar:
            for portfolio, task in tasks:
                try:
                    holdings_data = await task
                    pbar.set_postfix(ç»„åˆ=portfolio['name'][:10])
                    
                    if holdings_data and holdings_data.get('data'):
                        # è§£æè°ƒä»“è®°å½•
                        for record in holdings_data['data']:
                            tzrq = record.get('tzrq', '')
                            
                            # åªè¦ç›®æ ‡æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                            if tzrq in target_dates:
                                cwhj_mr = record.get('cwhj_mr', '-')
                                cwhj_mc = record.get('cwhj_mc', '-')
                                
                                # è§£æäº¤æ˜“åŠ¨ä½œ
                                action = self.parse_trading_action(cwhj_mr, cwhj_mc)
                                
                                if action:
                                    result_record = {
                                        'æ—¥æœŸ': tzrq,
                                        'ç»„åˆID': portfolio['id'],
                                        'ç»„åˆåç§°': portfolio['name'],
                                        'è°ƒä»“è‚¡ç¥¨': record.get('stkMktCode', ''),
                                        'è‚¡ç¥¨åç§°': record.get('stkName', ''),
                                        'è°ƒä»“æƒ…å†µ': action,
                                        'æˆäº¤ä»·æ ¼': record.get('cjjg_mr' if action == 'ä¹°å…¥' else 'cjjg_mc', '-'),
                                        'æŒä»“æ¯”ä¾‹': cwhj_mr if action == 'ä¹°å…¥' else cwhj_mc
                                    }
                                    results.append(result_record)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†ç»„åˆ{portfolio['name']}è°ƒä»“è®°å½•å¤±è´¥: {str(e)}")
                
                pbar.update(1)
        
        # 5. è¾“å‡ºç»“æœç»Ÿè®¡
        elapsed_time = time.time() - start_time
        print(f"ğŸ“ˆ è°ƒä»“è®°å½•è·å–å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’, ç»„åˆ: {len(portfolios)}ä¸ª, è®°å½•: {len(results)}æ¡")
        
        return results
    
    async def get_stock_summary(self, days_back=0, max_days_search=3):
        """
        è·å–è‚¡ç¥¨è°ƒä»“æ±‡æ€»æ•°æ®ï¼ŒæŒ‰è‚¡ç¥¨æ±‡æ€»ä¹°å…¥ç»„åˆæ•°å’Œå–å‡ºç»„åˆæ•°
        
        Args:
            days_back: å¾€å‰æ¨å‡ å¤©
            max_days_search: æœ€å¤šæœç´¢å‡ å¤©çš„æ•°æ®
            
        Returns:
            list: è‚¡ç¥¨æ±‡æ€»æ•°æ®åˆ—è¡¨ï¼ŒæŒ‰ä¹°å…¥ç»„åˆæ•°æ’åº
        """
        # è·å–è°ƒä»“è®°å½•ï¼ˆå®æ—¶è¯·æ±‚ï¼Œä¸ä½¿ç”¨ç¼“å­˜ï¼‰
        records = await self.tc_list(days_back=days_back, max_days_search=max_days_search, use_cache=False)
        
        # è·å–ç»„åˆæ’åä¿¡æ¯
        cached_data = await self.get_cached_rank_list_with_auto_update()
        portfolio_ranks = {}
        if cached_data and cached_data.get('data'):
            for rank_type, rank_data in cached_data.get('data', {}).items():
                rank_name = self.rank_types.get(rank_type, {}).get('name', rank_type)
                for idx, item in enumerate(rank_data.get('data', []), 1):
                    portfolio_id = item.get('zjzh')
                    portfolio_name = item.get('zhuheName')
                    if portfolio_id and portfolio_name:
                        if portfolio_name not in portfolio_ranks:
                            portfolio_ranks[portfolio_name] = []
                        # ç²¾ç®€æ’åä¿¡æ¯ï¼š"æ—¥æ¦œç¬¬3å" -> "æ—¥æ¦œ3"
                        portfolio_ranks[portfolio_name].append(f"{rank_name}{idx}")
        
        # æŒ‰è‚¡ç¥¨æ±‡æ€»
        stock_summary = {}
        
        for record in records:
            stock_code = record['è°ƒä»“è‚¡ç¥¨']
            stock_name = record['è‚¡ç¥¨åç§°']
            action = record['è°ƒä»“æƒ…å†µ']
            portfolio_name = record['ç»„åˆåç§°']
            portfolio_id = record['ç»„åˆID']
            
            # åˆ›å»ºè‚¡ç¥¨é”®
            stock_key = f"{stock_code}_{stock_name}"
            
            if stock_key not in stock_summary:
                stock_summary[stock_key] = {
                    'è‚¡ç¥¨ä»£ç ': stock_code,
                    'è‚¡ç¥¨åç§°': stock_name,
                    'ä¹°å…¥ç»„åˆ': [],
                    'å–å‡ºç»„åˆ': [],
                    'ä¹°å…¥ç»„åˆæ•°': 0,
                    'å–å‡ºç»„åˆæ•°': 0
                }
            
            # è·å–ç»„åˆæ’åä¿¡æ¯å¹¶æ·»åŠ åˆ°æ˜¾ç¤ºåç§°ä¸­
            portfolio_rank_info = portfolio_ranks.get(portfolio_name, [])
            if portfolio_rank_info:
                # æ ¼å¼åŒ–ä¸ºå¸¦tagçš„HTMLæ ¼å¼
                tags_html = ''
                for rank in portfolio_rank_info:
                    # æå–æ¦œå•ç±»å‹ï¼ˆæ—¥æ¦œã€å‘¨æ¦œã€æœˆæ¦œã€å¹´æ¦œã€æ€»æ¦œï¼‰
                    rank_type = ''
                    if 'æ—¥æ¦œ' in rank:
                        rank_type = 'æ—¥æ¦œ'
                    elif 'å‘¨æ¦œ' in rank:
                        rank_type = 'å‘¨æ¦œ'
                    elif 'æœˆæ¦œ' in rank:
                        rank_type = 'æœˆæ¦œ'
                    elif 'å¹´æ¦œ' in rank:
                        rank_type = 'å¹´æ¦œ'
                    elif 'æ€»æ¦œ' in rank:
                        rank_type = 'æ€»æ¦œ'
                    
                    if rank_type:
                        tags_html += f'<span class="rank-tag" data-rank-type="{rank_type}">{rank}</span>'
                
                portfolio_display = f'<span class="combo-name" data-combo-id="{portfolio_id}" data-combo-name="{portfolio_name}">{portfolio_name}</span>{tags_html}'
            else:
                portfolio_display = f'<span class="combo-name" data-combo-id="{portfolio_id}" data-combo-name="{portfolio_name}">{portfolio_name}</span>'
            
            # ç»Ÿè®¡ä¹°å…¥å–å‡ºç»„åˆï¼ˆä½¿ç”¨åŸå§‹ç»„åˆåç§°è¿›è¡Œå»é‡ï¼Œæ˜¾ç¤ºæ—¶ä½¿ç”¨å¸¦æ ‡ç­¾çš„åç§°ï¼‰
            if action == 'ä¹°å…¥':
                if portfolio_name not in [name.split('<span')[0] for name in stock_summary[stock_key]['ä¹°å…¥ç»„åˆ']]:
                    stock_summary[stock_key]['ä¹°å…¥ç»„åˆ'].append(portfolio_display)
            elif action == 'å–å‡º':
                if portfolio_name not in [name.split('<span')[0] for name in stock_summary[stock_key]['å–å‡ºç»„åˆ']]:
                    stock_summary[stock_key]['å–å‡ºç»„åˆ'].append(portfolio_display)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶è®¡ç®—ç»„åˆæ•°
        result = []
        for stock_data in stock_summary.values():
            # å°†HTMLæ ¼å¼çš„ç»„åˆä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–æ•°ç»„
            buy_combos = []
            sell_combos = []
            
            # å¤„ç†ä¹°å…¥ç»„åˆ
            for combo_html in stock_data['ä¹°å…¥ç»„åˆ']:
                combo_info = self._parse_combo_html(combo_html)
                if combo_info:
                    buy_combos.append(combo_info)
            
            # å¤„ç†å–å‡ºç»„åˆ
            for combo_html in stock_data['å–å‡ºç»„åˆ']:
                combo_info = self._parse_combo_html(combo_html)
                if combo_info:
                    sell_combos.append(combo_info)
            
            # æ›´æ–°æ•°æ®ç»“æ„
            stock_data['ä¹°å…¥ç»„åˆ'] = buy_combos
            stock_data['å–å‡ºç»„åˆ'] = sell_combos
            stock_data['ä¹°å…¥ç»„åˆæ•°'] = len(buy_combos)
            stock_data['å–å‡ºç»„åˆæ•°'] = len(sell_combos)
            result.append(stock_data)
        
        # æŒ‰ä¹°å…¥ç»„åˆæ•°æ’åºï¼ˆé™åºï¼‰
        result.sort(key=lambda x: x['ä¹°å…¥ç»„åˆæ•°'], reverse=True)
        
        return result

    async def get_portfolio_detail(self, portfolio_id: str) -> Optional[Dict[str, Any]]:
        url = f"{self.base_url}?type=rt_zhuhe_detail72&zh={portfolio_id}"
        
        try:
            data = await self.fetch_with_retry(url)
            if data and data.get('result') == '0' and 'data' in data:
                portfolio_data = data['data']
                
                # å¤„ç†æŒä»“ä¿¡æ¯
                positions = portfolio_data.get('position', [])
                processed_positions = []
                
                for pos in positions:
                    # è®¡ç®—ç›ˆäºé‡‘é¢ï¼ˆå‡è®¾æŒ‰æ¯”ä¾‹è®¡ç®—ï¼‰
                    try:
                        current_price = float(pos.get('__zxjg', 0))
                        cost_price = float(pos.get('cbj', 0))
                        position_rate = float(pos.get('positionRateDetail', 0))
                        profit_rate = float(pos.get('webYkRate', 0))
                        
                        processed_pos = {
                            'è‚¡ç¥¨ä»£ç ': pos.get('__code', ''),
                            'è‚¡ç¥¨åç§°': pos.get('__name', ''),
                            'æˆæœ¬ä»·': f"{cost_price:.3f}",
                            'ç°ä»·': f"{current_price:.3f}",
                            'ç›ˆäºæ¯”': f"{profit_rate:.2f}%",
                            'ä»“ä½': f"{position_rate:.1f}%"
                        }
                        processed_positions.append(processed_pos)
                    except (ValueError, TypeError) as e:
                        logger.warning(f"å¤„ç†æŒä»“æ•°æ®æ—¶å‡ºé”™: {e}")
                        continue
                
                # è·å–ç»„åˆåŸºæœ¬ä¿¡æ¯
                detail = portfolio_data.get('detail', {})
                result = {
                    'ç»„åˆID': portfolio_id,
                    'ç»„åˆåç§°': detail.get('zuheName', ''),
                    'ç®¡ç†äºº': detail.get('uidNick', ''),
                    'æ€»æ”¶ç›Šç‡': detail.get('rate', ''),
                    'ä»Šæ—¥æ”¶ç›Šç‡': detail.get('rateDay', ''),
                    'æŒä»“ä¿¡æ¯': processed_positions
                }
                
                return result
            else:
                logger.warning(f"è·å–ç»„åˆè¯¦æƒ…å¤±è´¥: {data.get('message', 'Unknown error') if data else 'No data'}")
                return None
                
        except Exception as e:
            logger.error(f"è·å–ç»„åˆè¯¦æƒ…å¼‚å¸¸: {str(e)}")
            return None


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # ==================== é…ç½®åŒºåŸŸ ====================
    # æ§åˆ¶æ˜¯å¦å¯ç”¨å»é‡åŠŸèƒ½ï¼ˆTrue=å¯ç”¨ï¼ŒFalse=ç¦ç”¨ï¼‰
    ENABLE_DEDUPLICATION = True
    
    # æ•°æ®åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
    REFRESH_INTERVAL = 100
    
    # æ˜¯å¦å¯ç”¨äº¤æ˜“æ—¶é—´æ£€æŸ¥ï¼ˆTrue=ä»…äº¤æ˜“æ—¶é—´æ›´æ–°ï¼ŒFalse=ä»»ä½•æ—¶é—´éƒ½æ›´æ–°ï¼‰
    ENABLE_TRADING_TIME_CHECK = False
    
    # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
    CACHE_EXPIRE_HOURS = 1
    
    # ç¼“å­˜ç›®å½•è·¯å¾„
    CACHE_DIR = "./data/cache"
    # ================================================
    
    # æ ¹æ®é…ç½®åˆå§‹åŒ–çˆ¬è™«
    crawler = RankCrawler(cache_dir=CACHE_DIR, enable_deduplication=ENABLE_DEDUPLICATION)
    
    async def main():
        print("è‚¡ç¥¨ç»„åˆæ’è¡Œæ¦œçˆ¬è™«å¯åŠ¨")
        
        # ç¨‹åºå¯åŠ¨æ—¶æ‰§è¡Œé¦–æ¬¡æ›´æ–°
        logger.info("ç¨‹åºå¯åŠ¨ï¼Œæ‰§è¡Œé¦–æ¬¡æ•°æ®æ›´æ–°")
        success = await crawler.run_update()
        
        if success:
            # å¯åŠ¨å®šæ—¶ä»»åŠ¡
            print(f"å®šæ—¶ä»»åŠ¡å¯åŠ¨ - æ›´æ–°é—´éš”: {REFRESH_INTERVAL}ç§’")
            
            try:
                # ä½¿ç”¨é…ç½®çš„åˆ·æ–°é—´éš”
                crawler.setup_schedule(REFRESH_INTERVAL)
                crawler.start_scheduler()
            except KeyboardInterrupt:
                print("\næ­£åœ¨åœæ­¢...")
                crawler.stop_scheduler()
                print("ç¨‹åºå·²åœæ­¢")
        else:
            logger.error("é¦–æ¬¡æ•°æ®æ›´æ–°å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå¼‚å¸¸: {str(e)}")
    finally:
        logger.info("ç¨‹åºå·²é€€å‡º")