#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""


åŠŸèƒ½ï¼š
1. è·å–5ä¸ªæ’è¡Œæ¦œæ•°æ®ï¼ˆæ—¥æ¦œã€å‘¨æ¦œã€æœˆæ¦œã€å¹´æ¦œã€æ€»æ¦œï¼‰
2. ä½¿ç”¨DiskCacheç¼“å­˜æ•°æ®
3. æ·»åŠ æ’åä¿¡æ¯
4. å®šæ—¶æ‰§è¡Œï¼ˆäº¤æ˜“æ—¶é—´å†…æ¯30ç§’æ‰§è¡Œä¸€æ¬¡ï¼‰
5. æ˜¾ç¤ºè¿›åº¦æ¡å’Œç»Ÿè®¡ä¿¡æ¯
"""

import asyncio
import aiohttp
import time
import logging
import os
import json
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from tqdm import tqdm
import diskcache as dc
import schedule
import threading

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RankCrawler:
    def __init__(self, cache_dir: str = "data/cache", enable_deduplication: bool = True, max_concurrent_requests: int = 5):
        """åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
            enable_deduplication: æ˜¯å¦å¯ç”¨å»é‡åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
            max_concurrent_requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼Œé»˜è®¤ä¸º5ï¼Œé¿å…è¢«å°IP
        """
        self.base_url = "https://emdcspzhapi.dfcfs.cn/rtV1"
        self.cache_dir = cache_dir
        self.enable_deduplication = enable_deduplication
        self.max_concurrent_requests = max_concurrent_requests
        self.ensure_cache_dir()
        
        # åˆå§‹åŒ–DiskCache
        self.cache = dc.Cache(os.path.join(self.cache_dir, 'rank_cache'))
        
        # åˆå§‹åŒ–å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        
        # æ’è¡Œæ¦œç±»å‹é…ç½®
        count = 100
        self.rank_types = {
            "daily": {"rankType": "10005", "name": "æ—¥æ¦œ", "recCnt": count},
            "weekly": {"rankType": "10000", "name": "å‘¨æ¦œ", "recCnt": count},
            "monthly": {"rankType": "10001", "name": "æœˆæ¦œ", "recCnt": count},
            "yearly": {"rankType": "10003", "name": "å¹´æ¦œ", "recCnt": count},
            "total": {"rankType": "10004", "name": "æ€»æ¦œ", "recCnt": count}
        }
        
        self.session = None
        self.running = False
        
        logger.info(f"çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œå»é‡åŠŸèƒ½: {'å¯ç”¨' if self.enable_deduplication else 'ç¦ç”¨'}ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_concurrent_requests}")
        
    def ensure_cache_dir(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        
        æ£€æŸ¥ç¼“å­˜ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºè¯¥ç›®å½•ã€‚
        è¿™æ˜¯åˆå§‹åŒ–è¿‡ç¨‹ä¸­çš„é‡è¦æ­¥éª¤ï¼Œç¡®ä¿åç»­çš„ç¼“å­˜æ“ä½œèƒ½å¤Ÿæ­£å¸¸è¿›è¡Œã€‚
        """
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    async def create_session(self):
        """åˆ›å»ºHTTPä¼šè¯
        
        åˆ›å»ºä¸€ä¸ªå¼‚æ­¥HTTPä¼šè¯ï¼Œç”¨äºå‘é€ç½‘ç»œè¯·æ±‚ã€‚
        é…ç½®äº†è¿æ¥æ± é™åˆ¶å’Œè¶…æ—¶è®¾ç½®ä»¥ä¼˜åŒ–æ€§èƒ½å’Œç¨³å®šæ€§ã€‚
        
        é…ç½®å‚æ•°:
            - æ€»è¶…æ—¶æ—¶é—´: 30ç§’
            - è¿æ¥æ± æ€»é™åˆ¶: 100ä¸ªè¿æ¥
            - å•ä¸»æœºè¿æ¥é™åˆ¶: 30ä¸ªè¿æ¥
        """
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            connector = aiohttp.TCPConnector(limit=100, limit_per_host=30)
            self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
    
    async def close_session(self):
        """å…³é—­HTTPä¼šè¯
        
        å®‰å…¨åœ°å…³é—­HTTPä¼šè¯ï¼Œé‡Šæ”¾ç›¸å…³èµ„æºã€‚
        åœ¨ç¨‹åºç»“æŸæˆ–éœ€è¦é‡æ–°åˆ›å»ºä¼šè¯æ—¶è°ƒç”¨ã€‚
        """
        if self.session and not self.session.closed:
            await self.session.close()
            self.session = None
    
    async def fetch_with_retry(self, url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """å¸¦é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚
        
        å‘é€HTTP GETè¯·æ±‚å¹¶åœ¨å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ã€‚
        è¿™ä¸ªæ–¹æ³•æä¾›äº†ç½‘ç»œè¯·æ±‚çš„å®¹é”™æœºåˆ¶ï¼Œæé«˜äº†æ•°æ®è·å–çš„å¯é æ€§ã€‚
        ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¢«æœåŠ¡å™¨å°IPã€‚
        
        Args:
            url (str): è¦è¯·æ±‚çš„URLåœ°å€
            max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3æ¬¡
            
        Returns:
            Optional[Dict[str, Any]]: æˆåŠŸæ—¶è¿”å›JSONå“åº”æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
            
        Note:
            - æ¯æ¬¡é‡è¯•å‰ä¼šç­‰å¾…1ç§’
            - åªæœ‰å½“APIè¿”å›result='0'æ—¶æ‰è®¤ä¸ºè¯·æ±‚æˆåŠŸ
            - ä¼šè‡ªåŠ¨å¤„ç†JSONè§£æé”™è¯¯
            - ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡ï¼Œä¿æŠ¤æœåŠ¡å™¨èµ„æº
        """
        await self.create_session()
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
        async with self.semaphore:
            for attempt in range(max_retries):
                try:
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            text = await response.text()
                            try:
                                data = json.loads(text)
                                if isinstance(data, dict) and data.get('result') == '0':
                                    return data
                                else:
                                    logger.warning(f"APIè¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
                            except json.JSONDecodeError:
                                logger.warning(f"æ— æ³•è§£æJSONå“åº”: {text[:100]}...")
                        else:
                            logger.warning(f"HTTPè¯·æ±‚å¤±è´¥: {response.status}")
                except Exception as e:
                    logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
            
            # åœ¨ä¿¡å·é‡ä¿æŠ¤ä¸‹æ·»åŠ è¯·æ±‚é—´éš”ï¼Œè¿›ä¸€æ­¥é™ä½è¢«å°IPçš„é£é™©
            await asyncio.sleep(0.1)  # æ¯ä¸ªè¯·æ±‚é—´éš”100ms
        
        return None
    
    async def fetch_rank_data(self, rank_type: str, rank_config: Dict) -> Optional[Dict[str, Any]]:
        """è·å–å•ä¸ªæ’è¡Œæ¦œæ•°æ®ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
        
        æ ¹æ®æ’è¡Œæ¦œç±»å‹å’Œé…ç½®è·å–å¯¹åº”çš„ç»„åˆæ’è¡Œæ•°æ®ã€‚
        æ”¯æŒåˆ†é¡µè·å–ï¼Œå¯ä»¥è·å–è¶…è¿‡APIå•æ¬¡é™åˆ¶çš„æ•°æ®é‡ã€‚
        
        Args:
            rank_type (str): æ’è¡Œæ¦œç±»å‹æ ‡è¯†ç¬¦
                           (å¦‚ 'daily', 'weekly', 'monthly', 'yearly', 'total')
            rank_config (Dict): æ’è¡Œæ¦œé…ç½®ä¿¡æ¯ï¼ŒåŒ…å«:
                - rankType: APIéœ€è¦çš„æ’è¡Œæ¦œç±»å‹å‚æ•°
                - name: æ’è¡Œæ¦œä¸­æ–‡åç§°
                - recCnt: éœ€è¦è·å–çš„è®°å½•æ•°é‡
                           
        Returns:
            Optional[Dict[str, Any]]: æˆåŠŸæ—¶è¿”å›åŒ…å«æ’è¡Œæ•°æ®çš„å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
                è¿”å›æ ¼å¼: {
                    'result': '0',
                    'data': [æ’è¡Œæ•°æ®åˆ—è¡¨]
                }
                
        Note:
            - APIå•æ¬¡æœ€å¤šè¿”å›20æ¡æ•°æ®ï¼Œæœ¬æ–¹æ³•ä¼šè‡ªåŠ¨åˆ†é¡µè·å–
            - ä¼šä¸ºæ¯æ¡æ•°æ®æ·»åŠ rankã€rank_typeã€rank_nameå­—æ®µ
            - å¦‚æœæŸé¡µè¿”å›æ•°æ®å°‘äºè¯·æ±‚æ•°é‡ï¼Œä¼šåœæ­¢åˆ†é¡µ
        """
        all_data = []
        target_count = rank_config['recCnt']
        page_size = 20  # APIé™åˆ¶æ¯æ¬¡æœ€å¤š20æ¡
        current_page = 0
        
        while len(all_data) < target_count:
            # è®¡ç®—æœ¬æ¬¡è¯·æ±‚çš„æ•°é‡
            remaining = target_count - len(all_data)
            current_count = min(page_size, remaining)
            
            url = f"{self.base_url}?type=rt_get_rank&rankType={rank_config['rankType']}&recIdx={current_page}&recCnt={current_count}&rankid=0"
            
            data = await self.fetch_with_retry(url)
            if data and 'data' in data and len(data['data']) > 0:
                all_data.extend(data['data'])
                current_page += 1
                
                # å¦‚æœè¿”å›çš„æ•°æ®å°‘äºè¯·æ±‚çš„æ•°é‡ï¼Œè¯´æ˜å·²ç»åˆ°åº•äº†
                if len(data['data']) < current_count:
                    break
            else:
                # å¦‚æœè¯·æ±‚å¤±è´¥æˆ–æ²¡æœ‰æ•°æ®ï¼Œåœæ­¢åˆ†é¡µ
                break
        
        if all_data:
            # æ·»åŠ æ’åä¿¡æ¯
            for i, item in enumerate(all_data, 1):
                item['rank'] = i
                item['rank_type'] = rank_type
                item['rank_name'] = rank_config['name']
            
            # æ„é€ è¿”å›æ•°æ®æ ¼å¼
            result_data = {
                'result': '0',
                'data': all_data[:target_count]  # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
            }
            
            return result_data
    
    def _parse_combo_html(self, combo_html):
        """è§£æç»„åˆHTMLå­—ç¬¦ä¸²ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯
        
        ä»HTMLæ ¼å¼çš„ç»„åˆä¿¡æ¯ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼ŒåŒ…æ‹¬ç»„åˆIDã€åç§°ã€æ˜¾ç¤ºåç§°å’Œæ’åæ ‡ç­¾ã€‚
        è¿™æ˜¯å¤„ç†å‰ç«¯ä¼ é€’çš„ç»„åˆHTMLæ•°æ®çš„æ ¸å¿ƒè§£æå‡½æ•°ï¼Œç¡®ä¿æ•°æ®çš„å‡†ç¡®æå–å’Œæ ¼å¼åŒ–ã€‚
        
        Args:
            combo_html (str): HTMLæ ¼å¼çš„ç»„åˆä¿¡æ¯å­—ç¬¦ä¸²ï¼ŒåŒ…å«ç»„åˆçš„åŸºæœ¬ä¿¡æ¯å’Œæ’åæ ‡ç­¾
                             æ ¼å¼ç¤ºä¾‹: '<span class="combo-name" data-combo-id="123" data-combo-name="æµ‹è¯•ç»„åˆ">æµ‹è¯•ç»„åˆ</span><span class="rank-tag" data-rank-type="daily">æ—¥æ¦œç¬¬1</span>'
                             
        Returns:
            Optional[Dict[str, Any]]: æˆåŠŸæ—¶è¿”å›ç»“æ„åŒ–çš„ç»„åˆä¿¡æ¯ï¼Œå¤±è´¥æ—¶è¿”å›None
                è¿”å›çš„æ•°æ®ç»“æ„:
                {
                    'id': 'ç»„åˆID',
                    'name': 'ç»„åˆåŸå§‹åç§°', 
                    'display_name': 'æ˜¾ç¤ºåç§°',
                    'ranks': [
                        {
                            'type': 'æ’åç±»å‹(daily/weekly/monthlyç­‰)',
                            'text': 'æ’åæ–‡æœ¬(å¦‚"æ—¥æ¦œç¬¬1")'
                        },
                        ...
                    ]
                }
                
        Process:
            1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç»„åˆçš„åŸºæœ¬ä¿¡æ¯(IDã€åç§°ã€æ˜¾ç¤ºåç§°)
            2. æå–æ‰€æœ‰æ’åæ ‡ç­¾ä¿¡æ¯
            3. æ„é€ å¹¶è¿”å›ç»“æ„åŒ–çš„æ•°æ®å¯¹è±¡
            
        Note:
            - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æHTMLï¼Œç¡®ä¿å‡†ç¡®æå–æ•°æ®å±æ€§
            - æ”¯æŒå¤šä¸ªæ’åæ ‡ç­¾çš„è§£æ
            - å¦‚æœHTMLæ ¼å¼ä¸åŒ¹é…ï¼Œè¿”å›None
            - æ’åæ ‡ç­¾æ˜¯å¯é€‰çš„ï¼Œç»„åˆå¯èƒ½æ²¡æœ‰æ’åä¿¡æ¯
        """
        import re
        
        # æå–ç»„åˆåç§°å’ŒID
        combo_match = re.search(r'data-combo-id="([^"]+)"[^>]*data-combo-name="([^"]+)"[^>]*>([^<]+)', combo_html)
        if not combo_match:
            return None
        
        combo_id = combo_match.group(1)
        combo_name = combo_match.group(2)
        display_name = combo_match.group(3)
        
        # æå–æ’åæ ‡ç­¾
        ranks = []
        rank_matches = re.findall(r'<span class="rank-tag"[^>]*data-rank-type="([^"]+)"[^>]*>([^<]+)</span>', combo_html)
        for rank_type, rank_text in rank_matches:
            ranks.append({
                'type': rank_type,
                'text': rank_text
            })
        
        return {
            'id': combo_id,
            'name': combo_name,
            'display_name': display_name,
            'ranks': ranks
        }
    
    async def fetch_all_rank_data(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®
        
        å¹¶å‘è·å–æ‰€æœ‰é…ç½®çš„æ’è¡Œæ¦œæ•°æ®ï¼ŒåŒ…æ‹¬æ—¥æ¦œã€å‘¨æ¦œã€æœˆæ¦œã€å¹´æ¦œã€æ€»æ¦œç­‰ã€‚
        è¿™æ˜¯æ•°æ®è·å–çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä½¿ç”¨å¼‚æ­¥å¹¶å‘æé«˜è·å–æ•ˆç‡ï¼Œå¹¶æä¾›å®æ—¶è¿›åº¦æ˜¾ç¤ºã€‚
        
        Returns:
            Dict[str, Any]: åŒ…å«æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®çš„å­—å…¸ï¼Œæ ¼å¼ä¸º:
                {
                    'daily': {'result': '0', 'data': [...]},
                    'weekly': {'result': '0', 'data': [...]},
                    'monthly': {'result': '0', 'data': [...]},
                    'yearly': {'result': '0', 'data': [...]},
                    'total': {'result': '0', 'data': [...]}
                }
                
        Process:
            1. åˆ›å»ºHTTPä¼šè¯è¿æ¥
            2. ä¸ºæ¯ä¸ªæ’è¡Œæ¦œç±»å‹åˆ›å»ºå¼‚æ­¥è·å–ä»»åŠ¡
            3. å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œå®æ—¶æ›´æ–°è¿›åº¦æ¡
            4. æ”¶é›†æ‰€æœ‰æˆåŠŸè·å–çš„æ•°æ®
            5. å…³é—­HTTPä¼šè¯å¹¶è¿”å›ç»“æœ
            
        Note:
            - ä½¿ç”¨å¼‚æ­¥å¹¶å‘æé«˜æ•°æ®è·å–æ•ˆç‡
            - æ˜¾ç¤ºè¯¦ç»†çš„è¿›åº¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®Œæˆæ•°é‡å’Œè€—æ—¶
            - è‡ªåŠ¨å¤„ç†HTTPä¼šè¯çš„åˆ›å»ºå’Œå…³é—­
            - å¤±è´¥çš„è¯·æ±‚ä¸ä¼šå½±å“å…¶ä»–æ•°æ®çš„è·å–
            - è¿”å›çš„æ•°æ®ç»“æ„ä¸å•ä¸ªæ’è¡Œæ¦œè·å–ä¿æŒä¸€è‡´
            - é€‚ç”¨äºæ‰¹é‡æ•°æ®æ›´æ–°å’Œåˆå§‹åŒ–åœºæ™¯
            
        Example:
            >>> data = await crawler.fetch_all_rank_data()
            >>> print(f"è·å–åˆ° {len(data)} ä¸ªæ’è¡Œæ¦œçš„æ•°æ®")
            >>> for rank_type, rank_data in data.items():
            >>>     print(f"{rank_type}: {len(rank_data.get('data', []))} æ¡è®°å½•")
        """
        await self.create_session()
        
        results = {}
        total_tasks = len(self.rank_types)
        
        with tqdm(total=total_tasks, desc="fetch_all_rank_data è·å–æ’è¡Œæ¦œæ•°æ®", unit="ä¸ª") as pbar:
            start_time = time.time()
            
            # å¹¶å‘è·å–æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®
            tasks = []
            for rank_type, rank_config in self.rank_types.items():
                task = self.fetch_rank_data(rank_type, rank_config)
                tasks.append((rank_type, task))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for rank_type, task in tasks:
                data = await task
                if data:
                    results[rank_type] = data
                
                pbar.update(1)
                elapsed_time = time.time() - start_time
                pbar.set_postfix({
                    'å·²å®Œæˆ': f"{pbar.n}/{total_tasks}",
                    'è€—æ—¶': f"{elapsed_time:.2f}s"
                })
        
        await self.close_session()
        return results
    
    def deduplicate_rank_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ’è¡Œæ¦œæ•°æ®è¿›è¡Œå»é‡ï¼Œä¼˜å…ˆçº§ï¼šæ€»æ¦œ>å¹´æ¦œ>æœˆæ¦œ>å‘¨æ¦œ>æ—¥æ¦œ
        
        å¯¹å¤šä¸ªæ’è¡Œæ¦œä¸­çš„é‡å¤ç»„åˆè¿›è¡Œå»é‡å¤„ç†ï¼Œç¡®ä¿æ¯ä¸ªç»„åˆåªåœ¨ä¼˜å…ˆçº§æœ€é«˜çš„æ¦œå•ä¸­å‡ºç°ã€‚
        è¿™æ ·å¯ä»¥é¿å…åŒä¸€ä¸ªç»„åˆåœ¨å¤šä¸ªæ¦œå•ä¸­é‡å¤æ˜¾ç¤ºï¼Œæä¾›æ›´æ¸…æ™°çš„æ•°æ®è§†å›¾ã€‚
        
        Args:
            data (Dict[str, Any]): åŒ…å«å¤šä¸ªæ’è¡Œæ¦œæ•°æ®çš„å­—å…¸ï¼Œæ ¼å¼ä¸º:
                {
                    'daily': {'result': '0', 'data': [...]},
                    'weekly': {'result': '0', 'data': [...]},
                    ...
                }
                
        Returns:
            Dict[str, Any]: å»é‡åçš„æ’è¡Œæ¦œæ•°æ®ï¼Œä¿æŒåŸæœ‰çš„æ•°æ®ç»“æ„
            
        Note:
            - ä¼˜å…ˆçº§é¡ºåºï¼šæ€»æ¦œ(1) > å¹´æ¦œ(2) > æœˆæ¦œ(3) > å‘¨æ¦œ(4) > æ—¥æ¦œ(5)
            - ä½¿ç”¨zjzhå­—æ®µï¼ˆç»„åˆIDï¼‰ä½œä¸ºå»é‡çš„å”¯ä¸€æ ‡è¯†ç¬¦
            - å»é‡åä¼šé‡æ–°åˆ†é…æ¯ä¸ªæ¦œå•å†…çš„æ’ååºå·
            - ä¿æŒåŸæœ‰çš„æ•°æ®ç»“æ„å’Œå­—æ®µä¸å˜
        """
        # å®šä¹‰æ¦œå•ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        rank_priority = {
            'total': 1,   # æ€»æ¦œ
            'yearly': 2,  # å¹´æ¦œ
            'monthly': 3, # æœˆæ¦œ
            'weekly': 4,  # å‘¨æ¦œ
            'daily': 5    # æ—¥æ¦œ
        }
        
        # å­˜å‚¨æ‰€æœ‰ç»„åˆæ•°æ®ï¼Œkeyä¸ºç»„åˆIDï¼Œvalueä¸º(ä¼˜å…ˆçº§, æ¦œå•ç±»å‹, æ•°æ®)
        portfolio_map = {}
        
        # éå†æ‰€æœ‰æ¦œå•æ•°æ®
        for rank_type, rank_data in data.items():
            priority = rank_priority.get(rank_type, 999)  # æœªçŸ¥ç±»å‹è®¾ä¸ºæœ€ä½ä¼˜å…ˆçº§
            
            for item in rank_data.get('data', []):
                portfolio_id = item.get('zjzh')  # ç»„åˆID
                if portfolio_id:
                    # å¦‚æœè¯¥ç»„åˆè¿˜æœªè®°å½•ï¼Œæˆ–å½“å‰æ¦œå•ä¼˜å…ˆçº§æ›´é«˜ï¼Œåˆ™æ›´æ–°è®°å½•
                    if (portfolio_id not in portfolio_map or 
                        priority < portfolio_map[portfolio_id][0]):
                        portfolio_map[portfolio_id] = (priority, rank_type, item)
        
        # é‡æ–°æ„å»ºå»é‡åçš„æ•°æ®ç»“æ„
        deduplicated_data = {}
        for rank_type in data.keys():
            deduplicated_data[rank_type] = {
                'result': '0',
                'data': []
            }
        
        # å°†å»é‡åçš„æ•°æ®åˆ†é…å›å¯¹åº”çš„æ¦œå•
        for portfolio_id, (priority, rank_type, item) in portfolio_map.items():
            deduplicated_data[rank_type]['data'].append(item)
        
        # é‡æ–°æ’åºæ¯ä¸ªæ¦œå•çš„æ•°æ®ï¼ˆæŒ‰åŸæœ‰æ’åï¼‰
        for rank_type, rank_data in deduplicated_data.items():
            rank_data['data'].sort(key=lambda x: x.get('rank', 999))
            # é‡æ–°åˆ†é…æ’å
            for i, item in enumerate(rank_data['data'], 1):
                item['rank'] = i
        
        return deduplicated_data
    
    def save_rank_data(self, data: Dict[str, Any]) -> str:
        """ä¿å­˜æ’è¡Œæ¦œæ•°æ®åˆ°DiskCache
        
        å°†è·å–åˆ°çš„æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜ä¸­ï¼Œæ”¯æŒæ•°æ®æŒä¹…åŒ–å’Œå¿«é€Ÿè®¿é—®ã€‚
        åŒæ—¶æä¾›æ•°æ®å»é‡ã€è¿‡æœŸæ¸…ç†ç­‰åŠŸèƒ½ï¼Œç¡®ä¿ç¼“å­˜æ•°æ®çš„è´¨é‡å’Œå­˜å‚¨æ•ˆç‡ã€‚
        
        Args:
            data (Dict[str, Any]): åŒ…å«æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®çš„å­—å…¸ï¼Œæ ¼å¼ä¸º:
                {
                    'daily': {'result': '0', 'data': [...]},
                    'weekly': {'result': '0', 'data': [...]},
                    ...
                }
                
        Returns:
            str: ç¼“å­˜é”®åï¼Œæ ¼å¼ä¸º 'rank_data_YYYYMMDD'
            
        Note:
            - æ ¹æ®enable_deduplicationé…ç½®å†³å®šæ˜¯å¦è¿›è¡Œå»é‡å¤„ç†
            - ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºç¼“å­˜é”®ï¼ŒåŒä¸€å¤©çš„æ•°æ®ä¼šè¦†ç›–ä¹‹å‰çš„ç¼“å­˜
            - åŒæ—¶ä¿å­˜ä¸ºlatest_rank_dataé”®ï¼Œä¾¿äºå¿«é€Ÿè®¿é—®æœ€æ–°æ•°æ®
            - è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´
            - è®°å½•è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ€»è®°å½•æ•°ã€æ›´æ–°æ—¶é—´ç­‰
        """
        date_str = datetime.now().strftime("%Y%m%d")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¿›è¡Œå»é‡å¤„ç†
        if self.enable_deduplication:
            deduplicated_data = self.deduplicate_rank_data(data)
        else:
            deduplicated_data = data
        
        # ç»Ÿè®¡æ€»è®°å½•æ•°
        total_records = sum(len(rank_data.get('data', [])) for rank_data in deduplicated_data.values())
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        merged_data = {
            "timestamp": timestamp,
            "date": datetime.now().strftime("%Y-%m-%d"),
            "update_time": datetime.now().isoformat(),
            "total_records": total_records,
            "data": deduplicated_data
        }
        
        # ä½¿ç”¨æ—¥æœŸä½œä¸ºç¼“å­˜é”®
        cache_key = f"rank_data_{date_str}"
        
        # æ¸…é™¤å½“å¤©çš„æ—§ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if cache_key in self.cache:
            del self.cache[cache_key]
            logger.info(f"ğŸ—‘ï¸ æ¸…é™¤æ—§ç¼“å­˜: {cache_key}")
        
        # ä¿å­˜åˆ°DiskCache
        self.cache.set(cache_key, merged_data)
        
        # ä¿å­˜æœ€æ–°æ•°æ®
        self.cache.set("latest_rank_data", merged_data)
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self.clean_expired_cache(CACHE_EXPIRE_HOURS)
        
        logger.info(f"ğŸ’¾ æ•°æ®å·²ç¼“å­˜: {cache_key}, è®°å½•æ•°: {total_records}æ¡")
        return cache_key
    
    def load_latest_rank_data(self) -> Optional[Dict[str, Any]]:
        """ä»DiskCacheåŠ è½½æœ€æ–°çš„æ’è¡Œæ¦œæ•°æ®
        
        ä»ç£ç›˜ç¼“å­˜ä¸­åŠ è½½æœ€æ–°ä¿å­˜çš„æ’è¡Œæ¦œæ•°æ®ï¼Œæä¾›å¿«é€Ÿçš„æ•°æ®è®¿é—®èƒ½åŠ›ã€‚
        è¿™ä¸ªæ–¹æ³•æ˜¯è·å–ç¼“å­˜æ•°æ®çš„ä¸»è¦å…¥å£ï¼Œæ”¯æŒç¦»çº¿æ•°æ®è®¿é—®å’Œå‡å°‘ç½‘ç»œè¯·æ±‚ã€‚
        
        Returns:
            Optional[Dict[str, Any]]: æˆåŠŸæ—¶è¿”å›å®Œæ•´çš„æ’è¡Œæ¦œæ•°æ®ç»“æ„ï¼Œå¤±è´¥æ—¶è¿”å›None
                è¿”å›æ ¼å¼: {
                    'timestamp': '20231201_143022',
                    'date': '2023-12-01',
                    'update_time': '2023-12-01T14:30:22.123456',
                    'total_records': 500,
                    'data': {
                        'daily': {'result': '0', 'data': [...]},
                        'weekly': {'result': '0', 'data': [...]},
                        ...
                    }
                }
                
        Note:
            - é€šè¿‡latest_rank_dataé”®è®¿é—®æœ€æ–°æ•°æ®
            - åŒ…å«å®Œæ•´çš„å…ƒæ•°æ®ä¿¡æ¯ï¼šæ—¶é—´æˆ³ã€æ—¥æœŸã€è®°å½•æ•°ç­‰
            - å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œè¿”å›None
            - å¼‚å¸¸æƒ…å†µä¼šè¢«æ•è·å¹¶è®°å½•åˆ°æ—¥å¿—ä¸­
        """
        try:
            data = self.cache.get("latest_rank_data")
            if data is not None:
                return data
        except Exception as e:
            logger.error(f"ä»DiskCacheåŠ è½½æ’è¡Œæ¦œæ•°æ®å¤±è´¥: {str(e)}")
        
        return None
    
    def clean_expired_cache(self, expire_hours: int = 1):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ•°æ®
        
        åˆ é™¤è¶…è¿‡æŒ‡å®šå°æ—¶æ•°çš„ç¼“å­˜æ•°æ®ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´å¹¶ä¿æŒç¼“å­˜çš„æ•´æ´ã€‚
        è¿™æ˜¯ä¸€ä¸ªç»´æŠ¤æ€§åŠŸèƒ½ï¼Œå®šæœŸæ¸…ç†å¯ä»¥é˜²æ­¢ç¼“å­˜æ— é™å¢é•¿ã€‚
        
        Args:
            expire_hours (int): ç¼“å­˜ä¿ç•™å°æ—¶æ•°ï¼Œé»˜è®¤ä¸º1å°æ—¶
                               è¶…è¿‡è¿™ä¸ªæ—¶é—´çš„ç¼“å­˜æ•°æ®å°†è¢«åˆ é™¤
                               
        Note:
            - åªæ¸…ç†ä»¥'rank_data_'å¼€å¤´çš„ç¼“å­˜é”®
            - æ”¯æŒä¸¤ç§æ—¥æœŸæ ¼å¼ï¼šYYYYMMDD å’Œ YYYYMMDD_HHMMSS
            - åŸºäºç¼“å­˜é”®ä¸­çš„æ—¶é—´æˆ³åˆ¤æ–­æ˜¯å¦è¿‡æœŸ
            - æ¸…ç†æ“ä½œæ˜¯å®‰å…¨çš„ï¼Œä¸ä¼šå½±å“æœ€æ–°æ•°æ®çš„è®¿é—®
            - ä¼šè®°å½•æ¸…ç†çš„ç¼“å­˜æ¡ç›®æ•°é‡
            - æ ¼å¼é”™è¯¯çš„ç¼“å­˜é”®ä¼šè¢«è·³è¿‡ï¼Œä¸ä¼šå½±å“æ¸…ç†æµç¨‹
        """
        try:
            current_time = datetime.now()
            expire_threshold = current_time - timedelta(hours=expire_hours)
            
            # è·å–æ‰€æœ‰ä»¥rank_data_å¼€å¤´çš„ç¼“å­˜é”®
            expired_keys = []
            for key in self.cache:
                if isinstance(key, str) and key.startswith('rank_data_'):
                    try:
                        # ä»é”®åä¸­æå–æ—¥æœŸ
                        date_str = key.replace('rank_data_', '')
                        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šYYYYMMDD å’Œ YYYYMMDD_HHMMSS
                        if len(date_str) == 8:  # æ–°æ ¼å¼ï¼šYYYYMMDD
                            cache_time = datetime.strptime(date_str, '%Y%m%d')
                        elif len(date_str) == 15:  # æ—§æ ¼å¼ï¼šYYYYMMDD_HHMMSS
                            cache_time = datetime.strptime(date_str, '%Y%m%d_%H%M%S')
                        else:
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                        if cache_time < expire_threshold:
                            expired_keys.append(key)
                    except ValueError:
                        # å¦‚æœæ—¶é—´æˆ³æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡
                        continue
            
            # åˆ é™¤è¿‡æœŸçš„ç¼“å­˜
            if expired_keys:
                for key in expired_keys:
                    del self.cache[key]
                logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)}ä¸ªé¡¹ç›®")
                
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {str(e)}")
    
    def is_trading_time(self) -> bool:
        """æ£€æŸ¥å½“å‰æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´
        
        åˆ¤æ–­å½“å‰æ—¶é—´æ˜¯å¦åœ¨è‚¡ç¥¨å¸‚åœºçš„äº¤æ˜“æ—¶é—´èŒƒå›´å†…ã€‚
        ç”¨äºæ§åˆ¶æ•°æ®æ›´æ–°çš„é¢‘ç‡ï¼Œé¿å…åœ¨éäº¤æ˜“æ—¶é—´è¿›è¡Œä¸å¿…è¦çš„æ•°æ®è·å–ã€‚
        
        Returns:
            bool: å¦‚æœå½“å‰æ—¶é—´åœ¨äº¤æ˜“æ—¶é—´å†…è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
            
        Note:
            äº¤æ˜“æ—¶é—´å®šä¹‰:
            - äº¤æ˜“æ—¥ï¼šå‘¨ä¸€è‡³å‘¨äº”ï¼ˆæ’é™¤å‘¨æœ«ï¼‰
            - ä¸Šåˆäº¤æ˜“æ—¶é—´ï¼š09:15 - 11:30
            - ä¸‹åˆäº¤æ˜“æ—¶é—´ï¼š13:00 - 15:00
            - ä¸è€ƒè™‘èŠ‚å‡æ—¥ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦é¢å¤–çš„èŠ‚å‡æ—¥åˆ¤æ–­
            - åŒ…å«é›†åˆç«ä»·æ—¶é—´ï¼ˆ09:15å¼€å§‹ï¼‰
        """
        now = datetime.now()
        current_time = now.time()
        
        # äº¤æ˜“æ—¶é—´ï¼š9:15-11:30, 13:00-15:00
        morning_start = datetime.strptime('09:15', '%H:%M').time()
        morning_end = datetime.strptime('11:30', '%H:%M').time()
        afternoon_start = datetime.strptime('13:00', '%H:%M').time()
        afternoon_end = datetime.strptime('15:00', '%H:%M').time()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå·¥ä½œæ—¥ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰
        if now.weekday() >= 5:  # å‘¨å…­ã€å‘¨æ—¥
            return False
            
        # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…
        is_morning = morning_start <= current_time <= morning_end
        is_afternoon = afternoon_start <= current_time <= afternoon_end
        
        return is_morning or is_afternoon
    
    async def run_update(self) -> bool:
        """æ‰§è¡Œä¸€æ¬¡æ•°æ®æ›´æ–°
        
        è·å–æ‰€æœ‰é…ç½®çš„æ’è¡Œæ¦œæ•°æ®å¹¶ä¿å­˜åˆ°ç¼“å­˜ä¸­ã€‚
        è¿™æ˜¯æ•°æ®æ›´æ–°çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè´Ÿè´£åè°ƒæ•´ä¸ªæ•°æ®è·å–å’Œå­˜å‚¨æµç¨‹ã€‚
        
        Returns:
            bool: æ›´æ–°æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
            
        Process:
            1. è°ƒç”¨fetch_all_rank_data()è·å–æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®
            2. å¦‚æœæ•°æ®è·å–æˆåŠŸï¼Œè°ƒç”¨save_rank_data()ä¿å­˜åˆ°ç¼“å­˜
            3. è®°å½•è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ï¼ˆè€—æ—¶ã€æ¦œå•æ•°ã€è®°å½•æ•°ï¼‰
            4. å¼‚å¸¸å¤„ç†ç¡®ä¿ç¨‹åºç¨³å®šæ€§
            
        Note:
            - ä¼šè®°å½•å¼€å§‹å’Œå®Œæˆçš„æ—¥å¿—ä¿¡æ¯
            - ç»Ÿè®¡å¹¶æ˜¾ç¤ºè·å–çš„æ¦œå•æ•°é‡å’Œæ€»è®°å½•æ•°
            - è®¡ç®—å¹¶è®°å½•æ•´ä¸ªæ›´æ–°è¿‡ç¨‹çš„è€—æ—¶
            - æ‰€æœ‰å¼‚å¸¸éƒ½ä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä¸ä¼šä¸­æ–­ç¨‹åºè¿è¡Œ
            - ç”¨äºå®šæ—¶ä»»åŠ¡å’Œæ‰‹åŠ¨æ›´æ–°ä¸¤ç§åœºæ™¯
        """
        try:
            logger.info("ğŸŒ æˆ‘æ˜¯å®šæ—¶==ä»»åŠ¡ï¼Œå¼€å§‹å®æ—¶è¯·æ±‚æ’è¡Œæ¦œæ•°æ®...")
            start_time = time.time()
            
            print("run_updateè°ƒç”¨çš„")
            # è·å–æ‰€æœ‰æ’è¡Œæ¦œæ•°æ®
            rank_data = await self.fetch_all_rank_data()
            
            if rank_data:
                # ä¿å­˜æ•°æ®
                cache_key = self.save_rank_data(rank_data)
                
                elapsed_time = time.time() - start_time
                total_records = sum(len(data.get('data', [])) for data in rank_data.values())
                
                logger.info(f"ğŸŒ æˆ‘æ˜¯å®šæ—¶==ä»»åŠ¡å®æ—¶æ•°æ®è·å–å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’, æ¦œå•: {len(rank_data)}ä¸ª, è®°å½•: {total_records}æ¡")
                
                return True
            else:
                logger.error("æœªè·å–åˆ°ä»»ä½•æ’è¡Œæ¦œæ•°æ®")
                return False
                
        except Exception as e:
            logger.error(f"æ•°æ®æ›´æ–°å¤±è´¥: {str(e)}")
            return False
    
    def update_job(self):
        """å®šæ—¶æ›´æ–°ä»»åŠ¡
        
        å®šæ—¶ä»»åŠ¡çš„å…¥å£æ–¹æ³•ï¼Œç”±è°ƒåº¦å™¨å®šæœŸè°ƒç”¨ã€‚
        åŒ…å«äº¤æ˜“æ—¶é—´æ£€æŸ¥å’Œå¼‚æ­¥ä»»åŠ¡çš„åŒæ­¥æ‰§è¡Œé€»è¾‘ã€‚
        
        Note:
            - é¦–å…ˆæ£€æŸ¥å½“å‰æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´ï¼Œéäº¤æ˜“æ—¶é—´ä¼šè·³è¿‡æ›´æ–°
            - åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥æ‰§è¡Œå¼‚æ­¥çš„æ•°æ®æ›´æ–°ä»»åŠ¡
            - ç¡®ä¿äº‹ä»¶å¾ªç¯åœ¨ä½¿ç”¨åè¢«æ­£ç¡®å…³é—­ï¼Œé¿å…èµ„æºæ³„æ¼
            - æ‰€æœ‰å¼‚å¸¸éƒ½ä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä¸ä¼šå½±å“è°ƒåº¦å™¨çš„æ­£å¸¸è¿è¡Œ
            - é€‚ç”¨äºAPSchedulerç­‰å®šæ—¶ä»»åŠ¡æ¡†æ¶
            
        Process:
            1. æ£€æŸ¥äº¤æ˜“æ—¶é—´
            2. åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
            3. åœ¨äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œrun_update()
            4. è®°å½•æ‰§è¡Œç»“æœ
            5. æ¸…ç†äº‹ä»¶å¾ªç¯èµ„æº
        """
        if ENABLE_TRADING_TIME_CHECK and not self.is_trading_time():
            logger.info("å½“å‰éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡æ•°æ®æ›´æ–°")
            return
            
        logger.info("å¼€å§‹å®šæ—¶æ•°æ®æ›´æ–°...")
        
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        def run_async_update():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                success = loop.run_until_complete(self.run_update())
                if success:
                    logger.info("å®šæ—¶æ•°æ®æ›´æ–°æˆåŠŸ")
                else:
                    logger.warning("å®šæ—¶æ•°æ®æ›´æ–°å¤±è´¥")
            except Exception as e:
                logger.error(f"å®šæ—¶æ•°æ®æ›´æ–°å¼‚å¸¸: {str(e)}")
            finally:
                loop.close()
        
        # åœ¨çº¿ç¨‹ä¸­è¿è¡Œä»¥é¿å…é˜»å¡
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            try:
                future = executor.submit(run_async_update)
                future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
            except concurrent.futures.TimeoutError:
                logger.error("å®šæ—¶æ›´æ–°ä»»åŠ¡è¶…æ—¶")
            except Exception as e:
                logger.error(f"å®šæ—¶æ›´æ–°ä»»åŠ¡å¤±è´¥: {str(e)}")
    
    def setup_schedule(self, refresh_interval=30):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡
        
        é…ç½®å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ï¼Œè®¾ç½®æ•°æ®æ›´æ–°çš„æ‰§è¡Œé—´éš”å’Œè°ƒåº¦ç­–ç•¥ã€‚
        è¿™æ˜¯å®šæ—¶æ•°æ®æ›´æ–°ç³»ç»Ÿçš„é…ç½®å…¥å£ï¼Œæ”¯æŒçµæ´»çš„æ—¶é—´é—´éš”è®¾ç½®ã€‚
        
        Args:
            refresh_interval (int): æ•°æ®åˆ·æ–°é—´éš”ï¼Œå•ä½ä¸ºç§’ï¼Œé»˜è®¤30ç§’
                                   å»ºè®®è®¾ç½®èŒƒå›´ï¼š10-300ç§’
                                   è¿‡çŸ­å¯èƒ½å¯¼è‡´é¢‘ç¹è¯·æ±‚ï¼Œè¿‡é•¿å¯èƒ½æ•°æ®ä¸å¤Ÿå®æ—¶
                                   
        Note:
            - ä½¿ç”¨scheduleåº“è¿›è¡Œä»»åŠ¡è°ƒåº¦
            - ä»»åŠ¡ä¼šåœ¨äº¤æ˜“æ—¶é—´å†…æŒ‰æŒ‡å®šé—´éš”æ‰§è¡Œ
            - éäº¤æ˜“æ—¶é—´ä¼šè‡ªåŠ¨è·³è¿‡æ›´æ–°ï¼ˆå¦‚æœå¯ç”¨äº¤æ˜“æ—¶é—´æ£€æŸ¥ï¼‰
            - æ¯æ¬¡è°ƒç”¨ä¼šæ¸…é™¤ä¹‹å‰çš„è°ƒåº¦è®¾ç½®
            - éœ€è¦è°ƒç”¨start_scheduler()æ‰èƒ½å¼€å§‹æ‰§è¡Œ
            
        Process:
            1. æ¸…é™¤ç°æœ‰çš„è°ƒåº¦ä»»åŠ¡
            2. è®¾ç½®æ–°çš„å®šæ—¶ä»»åŠ¡
            3. è®°å½•è°ƒåº¦é…ç½®ä¿¡æ¯
            
        Example:
            >>> crawler.setup_schedule(60)  # æ¯60ç§’æ›´æ–°ä¸€æ¬¡
            >>> crawler.start_scheduler()   # å¼€å§‹æ‰§è¡Œè°ƒåº¦
        """
        # äº¤æ˜“æ—¶é—´å†…æŒ‰æŒ‡å®šé—´éš”æ‰§è¡Œ
        schedule.every(refresh_interval).seconds.do(self.update_job)
        
        logger.info(f"å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼šäº¤æ˜“æ—¶é—´å†…æ¯{refresh_interval}ç§’æ›´æ–°ä¸€æ¬¡æ•°æ®")
        logger.info("äº¤æ˜“æ—¶é—´ï¼šå‘¨ä¸€è‡³å‘¨äº” 9:15-11:30, 13:00-15:00")
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        
        å¼€å§‹æ‰§è¡Œå®šæ—¶ä»»åŠ¡è°ƒåº¦ï¼Œè¿›å…¥æŒç»­è¿è¡ŒçŠ¶æ€ç›´åˆ°è¢«åœæ­¢ã€‚
        è¿™æ˜¯å®šæ—¶æ•°æ®æ›´æ–°ç³»ç»Ÿçš„æ‰§è¡Œå…¥å£ï¼Œä¼šé˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°è°ƒåº¦å™¨è¢«åœæ­¢ã€‚
        
        Note:
            - è¿™æ˜¯ä¸€ä¸ªé˜»å¡æ–¹æ³•ï¼Œä¼šæŒç»­è¿è¡Œç›´åˆ°stop_scheduler()è¢«è°ƒç”¨
            - æ¯ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡
            - è®¾ç½®runningæ ‡å¿—ä¸ºTrueï¼Œè¡¨ç¤ºè°ƒåº¦å™¨å¤„äºæ´»è·ƒçŠ¶æ€
            - é€‚åˆåœ¨ä¸»çº¿ç¨‹ä¸­è¿è¡Œï¼Œæˆ–è€…ä½¿ç”¨start_scheduler_in_thread()åœ¨åå°è¿è¡Œ
            - æ”¯æŒä¼˜é›…åœæ­¢ï¼Œé€šè¿‡runningæ ‡å¿—æ§åˆ¶å¾ªç¯é€€å‡º
            
        Process:
            1. è®¾ç½®è¿è¡ŒçŠ¶æ€æ ‡å¿—
            2. è®°å½•å¯åŠ¨æ—¥å¿—
            3. è¿›å…¥è°ƒåº¦å¾ªç¯ï¼Œæ¯ç§’æ£€æŸ¥å¾…æ‰§è¡Œä»»åŠ¡
            4. æ‰§è¡Œåˆ°æœŸçš„ä»»åŠ¡
            5. ç›´åˆ°runningæ ‡å¿—è¢«è®¾ä¸ºFalseæ—¶é€€å‡º
            
        Example:
            >>> crawler.setup_schedule(60)
            >>> crawler.start_scheduler()  # é˜»å¡è¿è¡Œ
            
            # æˆ–è€…åœ¨åå°è¿è¡Œ
            >>> crawler.start_scheduler_in_thread()
        """
        self.running = True
        
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")
        
        while self.running:
            schedule.run_pending()
            time.sleep(1)
    
    def stop_scheduler(self):
        """åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        
        ä¼˜é›…åœ°åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ï¼Œæ¸…ç†æ‰€æœ‰è°ƒåº¦ä»»åŠ¡å’Œèµ„æºã€‚
        è¿™æ˜¯å®šæ—¶æ•°æ®æ›´æ–°ç³»ç»Ÿçš„åœæ­¢å…¥å£ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿå®‰å…¨é€€å‡ºã€‚
        
        Note:
            - è®¾ç½®runningæ ‡å¿—ä¸ºFalseï¼Œä½¿è°ƒåº¦å¾ªç¯é€€å‡º
            - æ¸…é™¤æ‰€æœ‰å·²è®¾ç½®çš„å®šæ—¶ä»»åŠ¡
            - ä¸ä¼šä¸­æ–­æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼Œç­‰å¾…å…¶è‡ªç„¶å®Œæˆ
            - å¯ä»¥å®‰å…¨åœ°å¤šæ¬¡è°ƒç”¨ï¼Œä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨
            - åœæ­¢åå¯ä»¥é‡æ–°è°ƒç”¨setup_schedule()å’Œstart_scheduler()é‡å¯
            
        Process:
            1. è®¾ç½®è¿è¡ŒçŠ¶æ€æ ‡å¿—ä¸ºFalse
            2. æ¸…é™¤æ‰€æœ‰è°ƒåº¦ä»»åŠ¡
            3. è®°å½•åœæ­¢æ—¥å¿—
            
        Example:
            >>> crawler.stop_scheduler()  # åœæ­¢è°ƒåº¦å™¨
            >>> # å¯ä»¥é‡æ–°å¯åŠ¨
            >>> crawler.setup_schedule(30)
            >>> crawler.start_scheduler()
        """
        self.running = False
        schedule.clear()
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")
    
    def start_scheduler_in_thread(self):
        """åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        
        åœ¨ç‹¬ç«‹çš„åå°çº¿ç¨‹ä¸­å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹ã€‚
        è¿™æ˜¯éé˜»å¡å¯åŠ¨è°ƒåº¦å™¨çš„ä¾¿æ·æ–¹æ³•ï¼Œé€‚ç”¨äºéœ€è¦åŒæ—¶å¤„ç†å…¶ä»–ä»»åŠ¡çš„åœºæ™¯ã€‚
        
        Returns:
            threading.Thread: è¿è¡Œè°ƒåº¦å™¨çš„çº¿ç¨‹å¯¹è±¡ï¼Œå¯ç”¨äºç›‘æ§çº¿ç¨‹çŠ¶æ€
            
        Note:
            - åˆ›å»ºå®ˆæŠ¤çº¿ç¨‹(daemon=True)ï¼Œä¸»ç¨‹åºé€€å‡ºæ—¶ä¼šè‡ªåŠ¨ç»ˆæ­¢
            - éé˜»å¡æ–¹æ³•ï¼Œè°ƒç”¨åç«‹å³è¿”å›ï¼Œè°ƒåº¦å™¨åœ¨åå°è¿è¡Œ
            - è¿”å›çš„çº¿ç¨‹å¯¹è±¡å¯ç”¨äºæ£€æŸ¥çº¿ç¨‹çŠ¶æ€æˆ–ç­‰å¾…çº¿ç¨‹ç»“æŸ
            - è°ƒåº¦å™¨ä»ç„¶å¯ä»¥é€šè¿‡stop_scheduler()æ–¹æ³•åœæ­¢
            - é€‚åˆåœ¨Webåº”ç”¨æˆ–GUIåº”ç”¨ä¸­ä½¿ç”¨
            
        Process:
            1. å®šä¹‰å†…éƒ¨å‡½æ•°åŒ…è£…start_scheduler()è°ƒç”¨
            2. åˆ›å»ºå®ˆæŠ¤çº¿ç¨‹
            3. å¯åŠ¨çº¿ç¨‹
            4. è®°å½•å¯åŠ¨æ—¥å¿—
            5. è¿”å›çº¿ç¨‹å¯¹è±¡
            
        Example:
            >>> crawler.setup_schedule(60)
            >>> thread = crawler.start_scheduler_in_thread()
            >>> # ä¸»çº¿ç¨‹å¯ä»¥ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡
            >>> # ...
            >>> crawler.stop_scheduler()  # åœæ­¢åå°è°ƒåº¦å™¨
            >>> thread.join()  # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        """
        def run_scheduler():
            self.start_scheduler()
        
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨")
        return scheduler_thread
    
    def print_latest_data_summary(self):
        """æ‰“å°æœ€æ–°æ•°æ®æ‘˜è¦
        
        ä»ç¼“å­˜ä¸­è¯»å–æœ€æ–°çš„æ’è¡Œæ¦œæ•°æ®å¹¶ä»¥æ ¼å¼åŒ–çš„æ–¹å¼æ‰“å°æ‘˜è¦ä¿¡æ¯ã€‚
        è¿™æ˜¯ä¸€ä¸ªä¾¿æ·çš„æ•°æ®æŸ¥çœ‹å·¥å…·ï¼Œç”¨äºå¿«é€Ÿäº†è§£å½“å‰ç¼“å­˜çš„æ•°æ®çŠ¶æ€å’Œå†…å®¹æ¦‚è§ˆã€‚
        
        Note:
            - æ˜¾ç¤ºæ•°æ®æ›´æ–°æ—¶é—´å’Œæ€»è®°å½•æ•°
            - æŒ‰æ¦œå•ç±»å‹åˆ†åˆ«æ˜¾ç¤ºè®°å½•æ•°é‡
            - å±•ç¤ºæ¯ä¸ªæ¦œå•çš„å‰3åç»„åˆä¿¡æ¯
            - å¦‚æœæ²¡æœ‰ç¼“å­˜æ•°æ®ï¼Œä¼šæ˜¾ç¤ºç›¸åº”æç¤º
            - è¾“å‡ºæ ¼å¼å‹å¥½ï¼Œä¾¿äºäººå·¥æŸ¥çœ‹
            
        Output Format:
            === æœ€æ–°æ’è¡Œæ¦œæ•°æ®æ‘˜è¦ ===
            æ›´æ–°æ—¶é—´: 2024-01-01T12:00:00
            æ€»è®°å½•æ•°: 500æ¡
              æ—¥æ¦œ: 100æ¡
                å‰3å:
                  1. ç»„åˆåç§° (ç´¯è®¡æ”¶ç›Šç‡: 15.5%)
                  2. ç»„åˆåç§° (ç´¯è®¡æ”¶ç›Šç‡: 12.3%)
                  3. ç»„åˆåç§° (ç´¯è®¡æ”¶ç›Šç‡: 10.8%)
              ...
            
        Example:
            >>> crawler.print_latest_data_summary()
        """
        data = self.load_latest_rank_data()
        if data:
            print(f"\n=== æœ€æ–°æ’è¡Œæ¦œæ•°æ®æ‘˜è¦ ===")
            print(f"æ›´æ–°æ—¶é—´: {data.get('update_time', 'Unknown')}")
            print(f"æ€»è®°å½•æ•°: {data.get('total_records', 0)}æ¡")
            
            for rank_type, rank_data in data.get('data', {}).items():
                rank_name = self.rank_types.get(rank_type, {}).get('name', rank_type)
                record_count = len(rank_data.get('data', []))
                print(f"  {rank_name}: {record_count}æ¡")
                
                # æ˜¾ç¤ºå‰3å
                if rank_data.get('data'):
                    print(f"    å‰3å:")
                    for i, item in enumerate(rank_data['data'][:3], 1):
                        name = item.get('zhuheName', 'Unknown')
                        rate = item.get('rateForApp', 'N/A')
                        rate_title = item.get('rateTitle', '')
                        print(f"      {i}. {name} ({rate_title}: {rate}%)")
            print("=" * 40)
        else:
            print("æš‚æ— ç¼“å­˜æ•°æ®")
    
    async def get_cached_rank_list_with_auto_update(self) -> Dict[str, Any]:
        """è¯»å–ç¼“å­˜ä¸­çš„ç»„åˆåˆ—è¡¨ï¼Œè‹¥è·å–çš„æ•°æ®ä¸º0ï¼Œå°è¯•é‡æ–°æ›´æ–°ç»„åˆåˆ—è¡¨
        
        æ™ºèƒ½ç¼“å­˜æ•°æ®è·å–æ–¹æ³•ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œç¼“å­˜æ— æ•ˆæ—¶è‡ªåŠ¨è§¦å‘å®æ—¶æ›´æ–°ã€‚
        è¿™æ˜¯ä¸€ä¸ªé«˜å¯ç”¨æ€§çš„æ•°æ®è·å–æ¥å£ï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½èƒ½æä¾›æœ‰æ•ˆçš„æ’è¡Œæ¦œæ•°æ®ã€‚
        
        Returns:
            Dict[str, Any]: æ’è¡Œæ¦œæ•°æ®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„:
                {
                    'timestamp': 'æ—¶é—´æˆ³',
                    'date': 'æ—¥æœŸ',
                    'update_time': 'æ›´æ–°æ—¶é—´',
                    'total_records': æ€»è®°å½•æ•°,
                    'data': {
                        'daily': {'result': '0', 'data': [...]},
                        'weekly': {'result': '0', 'data': [...]},
                        ...
                    }
                }
                
        Strategy:
            1. ä¼˜å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½æ•°æ®
            2. æ£€æŸ¥ç¼“å­˜æ•°æ®çš„æœ‰æ•ˆæ€§ï¼ˆè®°å½•æ•° > 0ï¼‰
            3. å¦‚æœç¼“å­˜æ— æ•ˆï¼Œè‡ªåŠ¨è§¦å‘å®æ—¶æ•°æ®æ›´æ–°
            4. è¿”å›æ›´æ–°åçš„æ•°æ®æˆ–ç©ºå­—å…¸
            
        Performance:
            - ç¼“å­˜å‘½ä¸­ï¼šé€šå¸¸ < 0.1ç§’
            - å®æ—¶æ›´æ–°ï¼šé€šå¸¸ 10-30ç§’ï¼ˆå–å†³äºç½‘ç»œå’Œæ•°æ®é‡ï¼‰
            - åŒ…å«è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡å’ŒçŠ¶æ€æç¤º
            
        Note:
            - è‡ªåŠ¨å¤„ç†ç¼“å­˜å¤±æ•ˆå’Œæ•°æ®ä¸ºç©ºçš„æƒ…å†µ
            - æä¾›å‹å¥½çš„è¿›åº¦æç¤ºå’Œé”™è¯¯å¤„ç†
            - å¼‚å¸¸æƒ…å†µä¸‹è¿”å›ç©ºå­—å…¸ï¼Œä¸ä¼šæŠ›å‡ºå¼‚å¸¸
            - é€‚ç”¨äºéœ€è¦é«˜å¯ç”¨æ€§çš„æ•°æ®è·å–åœºæ™¯
            - è®°å½•è¯¦ç»†çš„æ‰§è¡Œæ—¶é—´å’Œæ•°æ®ç»Ÿè®¡
            
        Example:
            >>> data = await crawler.get_cached_rank_list_with_auto_update()
            >>> if data:
            >>>     print(f"è·å–åˆ° {data['total_records']} æ¡è®°å½•")
        """
        import time
        start_time = time.time()
        
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½æ•°æ®
        cached_data = self.load_latest_rank_data()
        
        if cached_data:
            total_records = cached_data.get('total_records', 0)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if total_records > 0:
                elapsed_time = time.time() - start_time
                print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ•°æ® - è€—æ—¶: {elapsed_time:.2f}ç§’, æ•°æ®: {total_records}æ¡")
                return cached_data
        
        # å¦‚æœç¼“å­˜ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è·å–æ•°æ®
        try:
            print("ğŸ”„ ç¼“å­˜æ— æ•ˆï¼Œå¼€å§‹å®æ—¶è¯·æ±‚æ’è¡Œæ¦œæ•°æ®...")
            update_start = time.time()
            success = await self.run_update()
            
            if success:
                # é‡æ–°åŠ è½½æ›´æ–°åçš„æ•°æ®
                updated_data = self.load_latest_rank_data()
                if updated_data:
                    total_records = updated_data.get('total_records', 0)
                    elapsed_time = time.time() - update_start
                    print(f"âœ… å®æ—¶æ•°æ®è·å–å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’, æ•°æ®: {total_records}æ¡")
                    return updated_data
                else:
                    print("âŒ å®æ—¶è¯·æ±‚åä»æ— æ³•è·å–æ•°æ®")
                    return {}
            else:
                print("âŒ å®æ—¶æ•°æ®è·å–å¤±è´¥")
                return {}
                
        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"âŒ å®æ—¶æ•°æ®è·å–å¼‚å¸¸ - è€—æ—¶: {elapsed_time:.2f}ç§’, é”™è¯¯: {str(e)}")
            logger.error(f"é‡æ–°è·å–æ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    async def fetch_portfolio_holdings(self, portfolio_id: str, rec_count: int = 50) -> Optional[Dict[str, Any]]:
        """è·å–å•ä¸ªç»„åˆçš„è°ƒä»“è®°å½•
        
        ä»APIè·å–æŒ‡å®šæŠ•èµ„ç»„åˆçš„æŒä»“å˜åŒ–è®°å½•ï¼ŒåŒ…æ‹¬ä¹°å…¥ã€å–å‡ºç­‰äº¤æ˜“æ“ä½œã€‚
        è¿™æ˜¯è·å–ç»„åˆäº¤æ˜“æ˜ç»†çš„æ ¸å¿ƒæ–¹æ³•ï¼Œç”¨äºåˆ†æç»„åˆçš„è°ƒä»“è¡Œä¸ºå’Œäº¤æ˜“ç­–ç•¥ã€‚
        
        Args:
            portfolio_id (str): ç»„åˆIDï¼Œç”¨äºå”¯ä¸€æ ‡è¯†ä¸€ä¸ªæŠ•èµ„ç»„åˆ
            rec_count (int): è·å–è®°å½•æ•°é‡ï¼Œé»˜è®¤50æ¡
                           å»ºè®®èŒƒå›´ï¼š10-100æ¡
                           è¿‡å¤šå¯èƒ½å½±å“æ€§èƒ½ï¼Œè¿‡å°‘å¯èƒ½é—æ¼é‡è¦ä¿¡æ¯
                           
        Returns:
            Optional[Dict[str, Any]]: æˆåŠŸæ—¶è¿”å›è°ƒä»“è®°å½•æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
                è¿”å›çš„æ•°æ®ç»“æ„:
                {
                    'result': '0',  # æˆåŠŸæ ‡è¯†
                    'data': [
                        {
                            'tzrq': 'è°ƒä»“æ—¥æœŸ',
                            'stkMktCode': 'è‚¡ç¥¨ä»£ç ',
                            'stkName': 'è‚¡ç¥¨åç§°',
                            'cwhj_mr': 'ä¹°å…¥æŒä»“å˜åŒ–',
                            'cwhj_mc': 'å–å‡ºæŒä»“å˜åŒ–',
                            'cjjg_mr': 'ä¹°å…¥æˆäº¤ä»·æ ¼',
                            'cjjg_mc': 'å–å‡ºæˆäº¤ä»·æ ¼',
                            ...
                        },
                        ...
                    ]
                }
                
        Process:
            1. æ„é€ APIè¯·æ±‚URL
            2. å‘é€HTTPè¯·æ±‚è·å–è°ƒä»“æ•°æ®
            3. è§£æå“åº”æ•°æ®å¹¶éªŒè¯ç»“æœ
            4. è¿”å›ç»“æ„åŒ–çš„è°ƒä»“è®°å½•
            
        Note:
            - ä½¿ç”¨rt_hold_change72æ¥å£è·å–è°ƒä»“è®°å½•
            - ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¢«æœåŠ¡å™¨å°IP
            - è‡ªåŠ¨å¤„ç†HTTPä¼šè¯ç®¡ç†
            - åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
            - è¶…æ—¶è®¾ç½®ä¸º10ç§’ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
            - æ¯ä¸ªè¯·æ±‚é—´éš”100msï¼Œè¿›ä¸€æ­¥é™ä½è¢«å°IPçš„é£é™©
            - å¼‚å¸¸æƒ…å†µä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä¸ä¼šå½±å“å…¶ä»–ç»„åˆçš„å¤„ç†
            - ç”¨äºtc_listæ–¹æ³•çš„æ•°æ®è·å–
            
        Example:
            >>> data = await crawler.fetch_portfolio_holdings('12345', 30)
            >>> if data:
            >>>     records = data.get('data', [])
            >>>     print(f"è·å–åˆ° {len(records)} æ¡è°ƒä»“è®°å½•")
        """
        url = f"https://emdcspzhapi.dfcfs.cn/rtV1?type=rt_hold_change72&zh={portfolio_id}&recIdx=1&recCnt={rec_count}"
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
        async with self.semaphore:
            try:
                if not self.session:
                    await self.create_session()
                
                async with self.session.get(url, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data.get('result') == '0':
                            return data
                        else:
                            logger.warning(f"è·å–ç»„åˆ{portfolio_id}è°ƒä»“è®°å½•å¤±è´¥: {data.get('message', 'Unknown error')}")
                            return None
                    else:
                        logger.error(f"è·å–ç»„åˆ{portfolio_id}è°ƒä»“è®°å½•HTTPé”™è¯¯: {response.status}")
                        return None
                        
            except Exception as e:
                logger.error(f"è·å–ç»„åˆ{portfolio_id}è°ƒä»“è®°å½•å¼‚å¸¸: {str(e)}")
                return None
            finally:
                # åœ¨ä¿¡å·é‡ä¿æŠ¤ä¸‹æ·»åŠ è¯·æ±‚é—´éš”ï¼Œè¿›ä¸€æ­¥é™ä½è¢«å°IPçš„é£é™©
                await asyncio.sleep(0.1)  # æ¯ä¸ªè¯·æ±‚é—´éš”100ms
    
    def parse_trading_action(self, cwhj_mr: str, cwhj_mc: str) -> Optional[str]:
        """è§£æäº¤æ˜“åŠ¨ä½œï¼ˆä¹°å…¥/å–å‡ºï¼‰
        
        æ ¹æ®æŒä»“å˜åŒ–æ•°æ®åˆ¤æ–­äº¤æ˜“åŠ¨ä½œç±»å‹ï¼Œç”¨äºè¯†åˆ«ç»„åˆçš„ä¹°å…¥æˆ–å–å‡ºæ“ä½œã€‚
        è¿™æ˜¯äº¤æ˜“è®°å½•åˆ†æçš„æ ¸å¿ƒå‡½æ•°ï¼Œç¡®ä¿å‡†ç¡®è¯†åˆ«æŠ•èµ„ç»„åˆçš„äº¤æ˜“è¡Œä¸ºã€‚
        
        Args:
            cwhj_mr (str): æŒä»“å˜åŒ–ä¹°å…¥æ•°æ®ï¼Œæ¥è‡ªAPIçš„cwhj_mrå­—æ®µ
                          - é"-"å€¼è¡¨ç¤ºæœ‰ä¹°å…¥æ“ä½œ
                          - å¯èƒ½åŒ…å«ç™¾åˆ†æ¯”æ•°æ®ï¼ˆå¦‚"10.5%"ï¼‰æˆ–"æˆ"å­—ç¬¦
                          - "-"è¡¨ç¤ºæ— ä¹°å…¥æ“ä½œ
            cwhj_mc (str): æŒä»“å˜åŒ–å–å‡ºæ•°æ®ï¼Œæ¥è‡ªAPIçš„cwhj_mcå­—æ®µ
                          - é"-"å€¼è¡¨ç¤ºæœ‰å–å‡ºæ“ä½œ
                          - å¯èƒ½åŒ…å«ç™¾åˆ†æ¯”æ•°æ®ï¼ˆå¦‚"8.3%"ï¼‰æˆ–"æˆ"å­—ç¬¦
                          - "-"è¡¨ç¤ºæ— å–å‡ºæ“ä½œ
                          
        Returns:
            Optional[str]: äº¤æ˜“åŠ¨ä½œç±»å‹
                - "ä¹°å…¥": æ£€æµ‹åˆ°ä¹°å…¥æ“ä½œ
                - "å–å‡º": æ£€æµ‹åˆ°å–å‡ºæ“ä½œ  
                - None: æ— æœ‰æ•ˆçš„äº¤æ˜“æ“ä½œ
                
        Logic:
            1. ä¼˜å…ˆæ£€æŸ¥ä¹°å…¥æ“ä½œï¼šcwhj_mrä¸ä¸ºç©ºä¸”ä¸ä¸º"-"
            2. å…¶æ¬¡æ£€æŸ¥å–å‡ºæ“ä½œï¼šcwhj_mcä¸ä¸ºç©ºä¸”ä¸ä¸º"-"
            3. éƒ½ä¸æ»¡è¶³åˆ™è¿”å›None
            
        Note:
            - ä¹°å…¥æ“ä½œä¼˜å…ˆçº§é«˜äºå–å‡ºæ“ä½œ
            - åŒ…å«"æˆ"å­—ç¬¦çš„æ•°æ®ä¹Ÿè¢«è§†ä¸ºæœ‰æ•ˆæ“ä½œ
            - ç©ºå­—ç¬¦ä¸²å’Œ"-"éƒ½è¢«è§†ä¸ºæ— æ“ä½œ
            - ç”¨äºtc_listæ•°æ®å¤„ç†å’Œäº¤æ˜“è®°å½•ç»Ÿè®¡
        """
        # cwhj_mr è‹¥ä¸æ˜¯-ï¼Œåˆ™ä»£è¡¨ä¹°å…¥ï¼ˆåŒ…å«"æˆ"çš„æ¯”ä¾‹ä¹Ÿæ˜¯æœ‰æ•ˆçš„ä¹°å…¥æ“ä½œï¼‰
        if cwhj_mr and cwhj_mr != "-":
            return "ä¹°å…¥"
        
        # cwhj_mc è‹¥ä¸æ˜¯-ï¼Œåˆ™ä»£è¡¨å–å‡ºï¼ˆåŒ…å«"æˆ"çš„æ¯”ä¾‹ä¹Ÿæ˜¯æœ‰æ•ˆçš„å–å‡ºæ“ä½œï¼‰
        if cwhj_mc and cwhj_mc != "-":
            return "å–å‡º"
        
        return None
    
    def is_a_stock_trading_day(self, date: datetime) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAè‚¡äº¤æ˜“æ—¥
        
        Aè‚¡äº¤æ˜“æ—¥è§„åˆ™ï¼š
        - å‘¨ä¸€åˆ°å‘¨äº”
        - æ’é™¤æ³•å®šèŠ‚å‡æ—¥ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªæ’é™¤å‘¨æœ«ï¼‰
        """
        # å‘¨ä¸€=0, å‘¨æ—¥=6
        weekday = date.weekday()
        # Aè‚¡äº¤æ˜“æ—¥ï¼šå‘¨ä¸€åˆ°å‘¨äº”ï¼ˆ0-4ï¼‰
        return weekday < 5
    
    def get_last_trading_day(self, from_date: datetime) -> datetime:
        """è·å–æŒ‡å®šæ—¥æœŸä¹‹å‰çš„æœ€è¿‘ä¸€ä¸ªAè‚¡äº¤æ˜“æ—¥"""
        current_date = from_date
        while True:
            current_date = current_date - timedelta(days=1)
            if self.is_a_stock_trading_day(current_date):
                return current_date
    
    def get_target_date(self, days_back: int = 0) -> str:
        """è·å–ç›®æ ‡æ—¥æœŸï¼ˆæ ¹æ®Aè‚¡äº¤æ˜“æ—¥è§„åˆ™ï¼‰
        
        æ ¹æ®å½“å‰æ—¶é—´å’ŒAè‚¡äº¤æ˜“è§„åˆ™è®¡ç®—åº”è¯¥è·å–æ•°æ®çš„ç›®æ ‡æ—¥æœŸã€‚
        ä¸»è¦ç”¨äºè°ƒä»“è®°å½•æŸ¥è¯¢ï¼Œç¡®ä¿è·å–åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥æ•°æ®ã€‚
        
        Args:
            days_back (int): å¾€å‰æ¨å‡ å¤©å¼€å§‹è®¡ç®—ï¼Œé»˜è®¤ä¸º0ï¼ˆå½“å¤©ï¼‰
            
        Returns:
            str: ç›®æ ‡æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸ºYYYYMMDD
            
        Rules:
            - 0ç‚¹åˆ°8ç‚¹30åˆ†ï¼šè°ƒä»“æ—¥æœŸæ˜¯å‰ä¸€ä¸ªAè‚¡äº¤æ˜“æ—¥
            - 8ç‚¹30åˆ†åˆ°24ç‚¹ï¼šè°ƒä»“æ—¥æœŸæ˜¯å½“å¤©ï¼Œè‹¥å½“å¤©ä¸æ˜¯Aè‚¡äº¤æ˜“æ—¥ï¼Œåˆ™å–ä¸Šä¸€ä¸ªæœ€è¿‘çš„äº¤æ˜“æ—¥
            
        Note:
            - è€ƒè™‘äº†Aè‚¡çš„äº¤æ˜“æ—¶é—´ç‰¹ç‚¹å’Œæ•°æ®æ›´æ–°è§„å¾‹
            - è‡ªåŠ¨å¤„ç†å‘¨æœ«å’Œéäº¤æ˜“æ—¥çš„æƒ…å†µ
            - æ”¯æŒå†å²æ—¥æœŸæŸ¥è¯¢ï¼ˆé€šè¿‡days_backå‚æ•°ï¼‰
            - è¿”å›çš„æ—¥æœŸæ ¼å¼ä¸APIæ¥å£è¦æ±‚ä¿æŒä¸€è‡´
        """
        now = datetime.now()
        current_time = now.time()
        
        # åº”ç”¨days_backåç§»
        base_date = now - timedelta(days=days_back)
        
        # æ ¹æ®æ—¶é—´è§„åˆ™ç¡®å®šç›®æ ‡æ—¥æœŸ
        if current_time < datetime.strptime('08:30', '%H:%M').time():
            # 0ç‚¹åˆ°8ç‚¹30åˆ†ï¼šå–å‰ä¸€ä¸ªAè‚¡äº¤æ˜“æ—¥
            target_date = self.get_last_trading_day(base_date)
        else:
            # 8ç‚¹30åˆ†åˆ°24ç‚¹ï¼šå–å½“å¤©ï¼Œè‹¥ä¸æ˜¯äº¤æ˜“æ—¥åˆ™å–ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
            if self.is_a_stock_trading_day(base_date):
                target_date = base_date
            else:
                target_date = self.get_last_trading_day(base_date)
        
        return target_date.strftime('%Y%m%d')
    
    async def tc_list(self, days_back: int = 0, max_days_search: int = 3, use_cache: bool = True) -> List[Dict[str, Any]]:
        """è·å–ç»„åˆè°ƒä»“è®°å½•åˆ—è¡¨
        
        è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ‰€æœ‰æŠ•èµ„ç»„åˆçš„è°ƒä»“è®°å½•ï¼ŒåŒ…æ‹¬ä¹°å…¥å’Œå–å‡ºæ“ä½œã€‚
        è¿™æ˜¯è·å–å¸‚åœºè°ƒä»“åŠ¨æ€çš„æ ¸å¿ƒæ¥å£ï¼Œæ”¯æŒçµæ´»çš„æ—¶é—´èŒƒå›´å’Œç¼“å­˜ç­–ç•¥ã€‚
        
        Args:
            days_back (int): å¾€å‰æ¨å‡ å¤©å¼€å§‹æŸ¥æ‰¾ï¼Œé»˜è®¤ä¸º0ï¼ˆå½“å¤©æˆ–å‰ä¸€å¤©ï¼‰
            max_days_search (int): æœ€å¤šæœç´¢å‡ å¤©çš„æ•°æ®ï¼Œé»˜è®¤ä¸º3å¤©
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œé»˜è®¤ä¸ºTrue
                             Falseè¡¨ç¤ºå®æ—¶è¯·æ±‚æœ€æ–°æ•°æ®
                             
        Returns:
            List[Dict[str, Any]]: è°ƒä»“è®°å½•åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•åŒ…å«:
                {
                    'æ—¥æœŸ': '20231201',
                    'ç»„åˆID': 'portfolio_id',
                    'ç»„åˆåç§°': 'ç»„åˆåç§°',
                    'è°ƒä»“è‚¡ç¥¨': '000001',
                    'è‚¡ç¥¨åç§°': 'å¹³å®‰é“¶è¡Œ',
                    'è°ƒä»“æƒ…å†µ': 'ä¹°å…¥'/'å–å‡º',
                    'æˆäº¤ä»·æ ¼': '10.123',
                    'æŒä»“æ¯”ä¾‹': '15.5%'
                }
                
        Process:
            1. è·å–ç»„åˆåˆ—è¡¨æ•°æ®ï¼ˆç¼“å­˜æˆ–å®æ—¶ï¼‰
            2. è®¡ç®—ç›®æ ‡æ—¥æœŸèŒƒå›´
            3. æ”¶é›†æ‰€æœ‰ç»„åˆçš„IDå’Œåç§°
            4. å¼‚æ­¥å¹¶å‘è·å–æ¯ä¸ªç»„åˆçš„è°ƒä»“è®°å½•
            5. è§£æå’Œè¿‡æ»¤ç›®æ ‡æ—¥æœŸèŒƒå›´å†…çš„è®°å½•
            6. ç»Ÿè®¡å¹¶è¿”å›ç»“æœ
            
        Note:
            - ä½¿ç”¨å¼‚æ­¥å¹¶å‘æé«˜æ•°æ®è·å–æ•ˆç‡
            - æ”¯æŒè¿›åº¦æ¡æ˜¾ç¤ºè·å–è¿›åº¦
            - è‡ªåŠ¨å¤„ç†å¼‚å¸¸ï¼Œå•ä¸ªç»„åˆå¤±è´¥ä¸å½±å“æ•´ä½“ç»“æœ
            - æ ¹æ®Aè‚¡äº¤æ˜“è§„åˆ™æ™ºèƒ½è®¡ç®—ç›®æ ‡æ—¥æœŸ
        """
        import time
        start_time = time.time()
        
        # 1. è·å–ç»„åˆåˆ—è¡¨æ•°æ®
        if use_cache:
            print("ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ•°æ®è·å–ç»„åˆåˆ—è¡¨")
            cached_data = await self.get_cached_rank_list_with_auto_update()
        else:
            print("ğŸŒ tclistæ¥å£ï¼Œå®æ—¶è¯·æ±‚ç»„åˆåˆ—è¡¨æ•°æ®")
            print("tc_list_è°ƒç”¨çš„")
            cached_data = await self.fetch_all_rank_data()
            
            # å¦‚æœå®æ—¶è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜æ•°æ®
            if not cached_data or not cached_data.get('data'):
                print("âš ï¸ å®æ—¶è¯·æ±‚å¤±è´¥ï¼Œå›é€€åˆ°ç¼“å­˜æ•°æ®")
                cached_data = await self.get_cached_rank_list_with_auto_update()
        
        if not cached_data or not cached_data.get('data'):
            print("âŒ æ— æ³•è·å–ç»„åˆåˆ—è¡¨æ•°æ®")
            return []
        
        # 2. è·å–ç›®æ ‡æ—¥æœŸåˆ—è¡¨
        target_dates = []
        for i in range(max_days_search):
            date = self.get_target_date(days_back + i)
            target_dates.append(date)
        
        # 3. æ”¶é›†æ‰€æœ‰ç»„åˆIDå’Œåç§°
        portfolios = []
        for rank_type, rank_data in cached_data.get('data', {}).items():
            for item in rank_data.get('data', []):
                portfolio_id = item.get('zjzh')
                portfolio_name = item.get('zhuheName')
                if portfolio_id and portfolio_name:
                    portfolios.append({
                        'id': portfolio_id,
                        'name': portfolio_name
                    })
        
        # 4. å¼‚æ­¥è·å–æ‰€æœ‰ç»„åˆçš„è°ƒä»“è®°å½•
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = []
        for portfolio in portfolios:
            task = self.fetch_portfolio_holdings(portfolio['id'])
            tasks.append((portfolio, task))
        
        # æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
        results = []
        with tqdm(total=len(tasks), desc="è·å–è°ƒä»“è®°å½•", unit="ä¸ª") as pbar:
            for portfolio, task in tasks:
                try:
                    holdings_data = await task
                    pbar.set_postfix(ç»„åˆ=portfolio['name'][:10])
                    
                    if holdings_data and holdings_data.get('data'):
                        # è§£æè°ƒä»“è®°å½•
                        for record in holdings_data['data']:
                            tzrq = record.get('tzrq', '')
                            
                            # åªè¦ç›®æ ‡æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                            if tzrq in target_dates:
                                cwhj_mr = record.get('cwhj_mr', '-')
                                cwhj_mc = record.get('cwhj_mc', '-')
                                
                                # è§£æäº¤æ˜“åŠ¨ä½œ
                                action = self.parse_trading_action(cwhj_mr, cwhj_mc)
                                
                                if action:
                                    result_record = {
                                        'æ—¥æœŸ': tzrq,
                                        'ç»„åˆID': portfolio['id'],
                                        'ç»„åˆåç§°': portfolio['name'],
                                        'è°ƒä»“è‚¡ç¥¨': record.get('stkMktCode', ''),
                                        'è‚¡ç¥¨åç§°': record.get('stkName', ''),
                                        'è°ƒä»“æƒ…å†µ': action,
                                        'æˆäº¤ä»·æ ¼': record.get('cjjg_mr' if action == 'ä¹°å…¥' else 'cjjg_mc', '-'),
                                        'æŒä»“æ¯”ä¾‹': cwhj_mr if action == 'ä¹°å…¥' else cwhj_mc
                                    }
                                    results.append(result_record)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†ç»„åˆ{portfolio['name']}è°ƒä»“è®°å½•å¤±è´¥: {str(e)}")
                
                pbar.update(1)
        
        # 5. è¾“å‡ºç»“æœç»Ÿè®¡
        elapsed_time = time.time() - start_time
        print(f"ğŸ“ˆ è°ƒä»“è®°å½•è·å–å®Œæˆ - è€—æ—¶: {elapsed_time:.2f}ç§’, ç»„åˆ: {len(portfolios)}ä¸ª, è®°å½•: {len(results)}æ¡")
        
        return results
    
    async def get_stock_summary(self, days_back=0, max_days_search=3):
        """è·å–è‚¡ç¥¨è°ƒä»“æ±‡æ€»æ•°æ®
        
        æŒ‰è‚¡ç¥¨ç»´åº¦æ±‡æ€»è°ƒä»“æ•°æ®ï¼Œç»Ÿè®¡æ¯åªè‚¡ç¥¨çš„ä¹°å…¥å’Œå–å‡ºç»„åˆæ•°é‡ã€‚
        è¿™æ˜¯åˆ†æå¸‚åœºçƒ­ç‚¹å’Œèµ„é‡‘æµå‘çš„é‡è¦å·¥å…·ã€‚
        
        Args:
            days_back (int): å¾€å‰æ¨å‡ å¤©å¼€å§‹ç»Ÿè®¡ï¼Œé»˜è®¤ä¸º0
            max_days_search (int): æœ€å¤šæœç´¢å‡ å¤©çš„æ•°æ®ï¼Œé»˜è®¤ä¸º3å¤©
            
        Returns:
            List[Dict[str, Any]]: è‚¡ç¥¨æ±‡æ€»æ•°æ®åˆ—è¡¨ï¼ŒæŒ‰ä¹°å…¥ç»„åˆæ•°é™åºæ’åº
                æ¯æ¡è®°å½•åŒ…å«:
                {
                    'è‚¡ç¥¨ä»£ç ': '000001',
                    'è‚¡ç¥¨åç§°': 'å¹³å®‰é“¶è¡Œ',
                    'ä¹°å…¥ç»„åˆ': [ç»„åˆä¿¡æ¯åˆ—è¡¨],
                    'å–å‡ºç»„åˆ': [ç»„åˆä¿¡æ¯åˆ—è¡¨],
                    'ä¹°å…¥ç»„åˆæ•°': 5,
                    'å–å‡ºç»„åˆæ•°': 2
                }
                
        Process:
            1. è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„è°ƒä»“è®°å½•ï¼ˆå®æ—¶æ•°æ®ï¼‰
            2. è·å–ç»„åˆæ’åä¿¡æ¯ç”¨äºæ ‡ç­¾æ˜¾ç¤º
            3. æŒ‰è‚¡ç¥¨ä»£ç æ±‡æ€»ä¹°å…¥å’Œå–å‡ºæ“ä½œ
            4. ä¸ºç»„åˆåç§°æ·»åŠ æ’è¡Œæ¦œæ ‡ç­¾
            5. è®¡ç®—ç»Ÿè®¡æ•°æ®å¹¶æ’åº
            
        Note:
            - å¼ºåˆ¶ä½¿ç”¨å®æ—¶æ•°æ®ç¡®ä¿å‡†ç¡®æ€§
            - è‡ªåŠ¨å»é‡ç›¸åŒç»„åˆçš„é‡å¤æ“ä½œ
            - ç»„åˆåç§°åŒ…å«æ’è¡Œæ¦œæ ‡ç­¾ï¼ˆæ—¥æ¦œã€å‘¨æ¦œç­‰ï¼‰
            - ç»“æœæŒ‰ä¹°å…¥ç»„åˆæ•°é‡é™åºæ’åˆ—ï¼Œä¾¿äºå‘ç°çƒ­é—¨è‚¡ç¥¨
        """
        # è·å–è°ƒä»“è®°å½•ï¼ˆå®æ—¶è¯·æ±‚ï¼Œä¸ä½¿ç”¨ç¼“å­˜ï¼‰
        records = await self.tc_list(days_back=days_back, max_days_search=max_days_search, use_cache=False)
        
        # è·å–ç»„åˆæ’åä¿¡æ¯
        cached_data = await self.get_cached_rank_list_with_auto_update()
        portfolio_ranks = {}
        if cached_data and cached_data.get('data'):
            for rank_type, rank_data in cached_data.get('data', {}).items():
                rank_name = self.rank_types.get(rank_type, {}).get('name', rank_type)
                for idx, item in enumerate(rank_data.get('data', []), 1):
                    portfolio_id = item.get('zjzh')
                    portfolio_name = item.get('zhuheName')
                    if portfolio_id and portfolio_name:
                        if portfolio_name not in portfolio_ranks:
                            portfolio_ranks[portfolio_name] = []
                        # ç²¾ç®€æ’åä¿¡æ¯ï¼š"æ—¥æ¦œç¬¬3å" -> "æ—¥æ¦œ3"
                        portfolio_ranks[portfolio_name].append(f"{rank_name}{idx}")
        
        # æŒ‰è‚¡ç¥¨æ±‡æ€»
        stock_summary = {}
        
        for record in records:
            stock_code = record['è°ƒä»“è‚¡ç¥¨']
            stock_name = record['è‚¡ç¥¨åç§°']
            action = record['è°ƒä»“æƒ…å†µ']
            portfolio_name = record['ç»„åˆåç§°']
            portfolio_id = record['ç»„åˆID']
            
            # åˆ›å»ºè‚¡ç¥¨é”®
            stock_key = f"{stock_code}_{stock_name}"
            
            if stock_key not in stock_summary:
                stock_summary[stock_key] = {
                    'è‚¡ç¥¨ä»£ç ': stock_code,
                    'è‚¡ç¥¨åç§°': stock_name,
                    'ä¹°å…¥ç»„åˆ': [],
                    'å–å‡ºç»„åˆ': [],
                    'ä¹°å…¥ç»„åˆæ•°': 0,
                    'å–å‡ºç»„åˆæ•°': 0
                }
            
            # è·å–ç»„åˆæ’åä¿¡æ¯å¹¶æ·»åŠ åˆ°æ˜¾ç¤ºåç§°ä¸­
            portfolio_rank_info = portfolio_ranks.get(portfolio_name, [])
            if portfolio_rank_info:
                # æ ¼å¼åŒ–ä¸ºå¸¦tagçš„HTMLæ ¼å¼
                tags_html = ''
                for rank in portfolio_rank_info:
                    # æå–æ¦œå•ç±»å‹ï¼ˆæ—¥æ¦œã€å‘¨æ¦œã€æœˆæ¦œã€å¹´æ¦œã€æ€»æ¦œï¼‰
                    rank_type = ''
                    if 'æ—¥æ¦œ' in rank:
                        rank_type = 'æ—¥æ¦œ'
                    elif 'å‘¨æ¦œ' in rank:
                        rank_type = 'å‘¨æ¦œ'
                    elif 'æœˆæ¦œ' in rank:
                        rank_type = 'æœˆæ¦œ'
                    elif 'å¹´æ¦œ' in rank:
                        rank_type = 'å¹´æ¦œ'
                    elif 'æ€»æ¦œ' in rank:
                        rank_type = 'æ€»æ¦œ'
                    
                    if rank_type:
                        tags_html += f'<span class="rank-tag" data-rank-type="{rank_type}">{rank}</span>'
                
                portfolio_display = f'<span class="combo-name" data-combo-id="{portfolio_id}" data-combo-name="{portfolio_name}">{portfolio_name}</span>{tags_html}'
            else:
                portfolio_display = f'<span class="combo-name" data-combo-id="{portfolio_id}" data-combo-name="{portfolio_name}">{portfolio_name}</span>'
            
            # ç»Ÿè®¡ä¹°å…¥å–å‡ºç»„åˆï¼ˆä½¿ç”¨åŸå§‹ç»„åˆåç§°è¿›è¡Œå»é‡ï¼Œæ˜¾ç¤ºæ—¶ä½¿ç”¨å¸¦æ ‡ç­¾çš„åç§°ï¼‰
            if action == 'ä¹°å…¥':
                if portfolio_name not in [name.split('<span')[0] for name in stock_summary[stock_key]['ä¹°å…¥ç»„åˆ']]:
                    stock_summary[stock_key]['ä¹°å…¥ç»„åˆ'].append(portfolio_display)
            elif action == 'å–å‡º':
                if portfolio_name not in [name.split('<span')[0] for name in stock_summary[stock_key]['å–å‡ºç»„åˆ']]:
                    stock_summary[stock_key]['å–å‡ºç»„åˆ'].append(portfolio_display)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶è®¡ç®—ç»„åˆæ•°
        result = []
        for stock_data in stock_summary.values():
            # å°†HTMLæ ¼å¼çš„ç»„åˆä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–æ•°ç»„
            buy_combos = []
            sell_combos = []
            
            # å¤„ç†ä¹°å…¥ç»„åˆ
            for combo_html in stock_data['ä¹°å…¥ç»„åˆ']:
                combo_info = self._parse_combo_html(combo_html)
                if combo_info:
                    buy_combos.append(combo_info)
            
            # å¤„ç†å–å‡ºç»„åˆ
            for combo_html in stock_data['å–å‡ºç»„åˆ']:
                combo_info = self._parse_combo_html(combo_html)
                if combo_info:
                    sell_combos.append(combo_info)
            
            # æ›´æ–°æ•°æ®ç»“æ„
            stock_data['ä¹°å…¥ç»„åˆ'] = buy_combos
            stock_data['å–å‡ºç»„åˆ'] = sell_combos
            stock_data['ä¹°å…¥ç»„åˆæ•°'] = len(buy_combos)
            stock_data['å–å‡ºç»„åˆæ•°'] = len(sell_combos)
            result.append(stock_data)
        
        # æŒ‰ä¹°å…¥ç»„åˆæ•°æ’åºï¼ˆé™åºï¼‰
        result.sort(key=lambda x: x['ä¹°å…¥ç»„åˆæ•°'], reverse=True)
        
        return result

    async def get_portfolio_detail(self, portfolio_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç»„åˆè¯¦æƒ…
        
        æ ¹æ®ç»„åˆIDè·å–æŒ‡å®šæŠ•èµ„ç»„åˆçš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŸºæœ¬ä¿¡æ¯å’ŒæŒä»“æ˜ç»†ã€‚
        è¿™æ˜¯è·å–å•ä¸ªæŠ•èµ„ç»„åˆå®Œæ•´ä¿¡æ¯çš„ä¸»è¦æ¥å£ã€‚
        
        Args:
            portfolio_id (str): ç»„åˆIDï¼ˆzjzhå­—æ®µå€¼ï¼‰ï¼Œç”¨äºå”¯ä¸€æ ‡è¯†ä¸€ä¸ªæŠ•èµ„ç»„åˆ
            
        Returns:
            Optional[Dict[str, Any]]: æˆåŠŸæ—¶è¿”å›ç»„åˆçš„å®Œæ•´ä¿¡æ¯ï¼Œå¤±è´¥æˆ–æœªæ‰¾åˆ°æ—¶è¿”å›None
                  è¿”å›çš„æ•°æ®åŒ…å«ç»„åˆåŸºæœ¬ä¿¡æ¯å’ŒæŒä»“æ˜ç»†:
                  {
                      'ç»„åˆID': 'portfolio_id',
                      'ç»„åˆåç§°': 'ç»„åˆåç§°',
                      'ç®¡ç†äºº': 'ç®¡ç†äººæ˜µç§°',
                      'æ€»æ”¶ç›Šç‡': 'ç´¯è®¡æ”¶ç›Šç‡',
                      'ä»Šæ—¥æ”¶ç›Šç‡': 'å½“æ—¥æ”¶ç›Šç‡',
                      'æŒä»“ä¿¡æ¯': [
                          {
                              'è‚¡ç¥¨ä»£ç ': '000001',
                              'è‚¡ç¥¨åç§°': 'å¹³å®‰é“¶è¡Œ',
                              'æˆæœ¬ä»·': '10.123',
                              'ç°ä»·': '11.456',
                              'ç›ˆäºæ¯”': '13.12%',
                              'ä»“ä½': '15.5%'
                          },
                          ...
                      ]
                  }
                  
        Process:
            1. æ„é€ APIè¯·æ±‚URL
            2. å‘é€HTTPè¯·æ±‚è·å–ç»„åˆè¯¦æƒ…æ•°æ®
            3. è§£ææŒä»“ä¿¡æ¯ï¼Œè®¡ç®—æ ¼å¼åŒ–æ•°æ®
            4. æå–ç»„åˆåŸºæœ¬ä¿¡æ¯
            5. æ„é€ å¹¶è¿”å›æ ‡å‡†åŒ–çš„ç»“æœæ•°æ®
            
        Note:
            - ä½¿ç”¨rt_zhuhe_detail72æ¥å£è·å–è¯¦ç»†æ•°æ®
            - è‡ªåŠ¨å¤„ç†æ•°æ®ç±»å‹è½¬æ¢å’Œæ ¼å¼åŒ–
            - æŒä»“æ•°æ®åŒ…å«æˆæœ¬ä»·ã€ç°ä»·ã€ç›ˆäºæ¯”ä¾‹ã€ä»“ä½æ¯”ä¾‹ç­‰
            - å¼‚å¸¸æƒ…å†µä¼šè¢«æ•è·å¹¶è®°å½•åˆ°æ—¥å¿—ä¸­
            - æ•°æ®è§£æå¤±è´¥çš„æŒä»“è®°å½•ä¼šè¢«è·³è¿‡ï¼Œä¸å½±å“å…¶ä»–æ•°æ®
        """
        url = f"{self.base_url}?type=rt_zhuhe_detail72&zh={portfolio_id}"
        
        try:
            data = await self.fetch_with_retry(url)
            if data and data.get('result') == '0' and 'data' in data:
                portfolio_data = data['data']
                
                # å¤„ç†æŒä»“ä¿¡æ¯
                positions = portfolio_data.get('position', [])
                processed_positions = []
                
                for pos in positions:
                    # è®¡ç®—ç›ˆäºé‡‘é¢ï¼ˆå‡è®¾æŒ‰æ¯”ä¾‹è®¡ç®—ï¼‰
                    try:
                        current_price = float(pos.get('__zxjg', 0))
                        cost_price = float(pos.get('cbj', 0))
                        position_rate = float(pos.get('positionRateDetail', 0))
                        profit_rate = float(pos.get('webYkRate', 0))
                        
                        processed_pos = {
                            'è‚¡ç¥¨ä»£ç ': pos.get('__code', ''),
                            'è‚¡ç¥¨åç§°': pos.get('__name', ''),
                            'æˆæœ¬ä»·': f"{cost_price:.3f}",
                            'ç°ä»·': f"{current_price:.3f}",
                            'ç›ˆäºæ¯”': f"{profit_rate:.2f}%",
                            'ä»“ä½': f"{position_rate:.1f}%"
                        }
                        processed_positions.append(processed_pos)
                    except (ValueError, TypeError) as e:
                        logger.warning(f"å¤„ç†æŒä»“æ•°æ®æ—¶å‡ºé”™: {e}")
                        continue
                
                # è·å–ç»„åˆåŸºæœ¬ä¿¡æ¯
                detail = portfolio_data.get('detail', {})
                result = {
                    'ç»„åˆID': portfolio_id,
                    'ç»„åˆåç§°': detail.get('zuheName', ''),
                    'ç®¡ç†äºº': detail.get('uidNick', ''),
                    'æ€»æ”¶ç›Šç‡': detail.get('rate', ''),
                    'ä»Šæ—¥æ”¶ç›Šç‡': detail.get('rateDay', ''),
                    'æŒä»“ä¿¡æ¯': processed_positions
                }
                
                return result
            else:
                logger.warning(f"è·å–ç»„åˆè¯¦æƒ…å¤±è´¥: {data.get('message', 'Unknown error') if data else 'No data'}")
                return None
                
        except Exception as e:
            logger.error(f"è·å–ç»„åˆè¯¦æƒ…å¼‚å¸¸: {str(e)}")
            return None


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    """
    ä¸»ç¨‹åºå…¥å£
    
    å½“è„šæœ¬ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œçš„ä»£ç ï¼Œæä¾›å®Œæ•´çš„è‚¡ç¥¨ç»„åˆæ’è¡Œæ¦œæ•°æ®çˆ¬å–å’Œå®šæ—¶æ›´æ–°åŠŸèƒ½ã€‚
    æ”¯æŒçµæ´»çš„é…ç½®é€‰é¡¹ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´çˆ¬å–ç­–ç•¥ã€æ›´æ–°é¢‘ç‡å’Œç¼“å­˜è®¾ç½®ã€‚
    
    ä¸»è¦åŠŸèƒ½:
        1. é…ç½®ç³»ç»Ÿå‚æ•°ï¼ˆå»é‡ã€æ›´æ–°é—´éš”ã€äº¤æ˜“æ—¶é—´æ£€æŸ¥ç­‰ï¼‰
        2. åˆå§‹åŒ–RankCrawlerçˆ¬è™«å®ä¾‹
        3. æ‰§è¡Œé¦–æ¬¡æ•°æ®æ›´æ–°
        4. å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        5. æä¾›ä¼˜é›…çš„ç¨‹åºé€€å‡ºæœºåˆ¶
        
    é…ç½®é€‰é¡¹:
        - ENABLE_DEDUPLICATION: æ˜¯å¦å¯ç”¨æ•°æ®å»é‡åŠŸèƒ½
        - REFRESH_INTERVAL: æ•°æ®åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
        - ENABLE_TRADING_TIME_CHECK: æ˜¯å¦ä»…åœ¨äº¤æ˜“æ—¶é—´æ›´æ–°æ•°æ®
        - CACHE_EXPIRE_HOURS: ç¼“å­˜æ•°æ®è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
        - CACHE_DIR: ç¼“å­˜æ–‡ä»¶å­˜å‚¨ç›®å½•
        
    è¿è¡Œæµç¨‹:
        1. è¯»å–é…ç½®å‚æ•°
        2. åˆ›å»ºçˆ¬è™«å®ä¾‹
        3. æ‰§è¡Œé¦–æ¬¡æ•°æ®æ›´æ–°
        4. å¦‚æœé¦–æ¬¡æ›´æ–°æˆåŠŸï¼Œå¯åŠ¨å®šæ—¶ä»»åŠ¡
        5. æŒç»­è¿è¡Œç›´åˆ°ç”¨æˆ·ä¸­æ–­æˆ–å¼‚å¸¸é€€å‡º
        
    Note:
        - æ”¯æŒCtrl+Cä¼˜é›…é€€å‡º
        - æ‰€æœ‰å¼‚å¸¸éƒ½ä¼šè¢«æ•è·å¹¶è®°å½•
        - é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒçš„é•¿æœŸè¿è¡Œ
        - å¯ä»¥é€šè¿‡ä¿®æ”¹é…ç½®å‚æ•°è°ƒæ•´è¿è¡Œè¡Œä¸º
    """
    # ==================== é…ç½®åŒºåŸŸ ====================
    # æ§åˆ¶æ˜¯å¦å¯ç”¨å»é‡åŠŸèƒ½ï¼ˆTrue=å¯ç”¨ï¼ŒFalse=ç¦ç”¨ï¼‰
    ENABLE_DEDUPLICATION = True
    
    # æ•°æ®åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
    REFRESH_INTERVAL = 100
    
    # æ˜¯å¦å¯ç”¨äº¤æ˜“æ—¶é—´æ£€æŸ¥ï¼ˆTrue=ä»…äº¤æ˜“æ—¶é—´æ›´æ–°ï¼ŒFalse=ä»»ä½•æ—¶é—´éƒ½æ›´æ–°ï¼‰
    ENABLE_TRADING_TIME_CHECK = False
    
    # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
    CACHE_EXPIRE_HOURS = 1
    
    # ç¼“å­˜ç›®å½•è·¯å¾„
    CACHE_DIR = "./data/cache"
    # ================================================
    
    # æ ¹æ®é…ç½®åˆå§‹åŒ–çˆ¬è™«
    crawler = RankCrawler(cache_dir=CACHE_DIR, enable_deduplication=ENABLE_DEDUPLICATION)
    
    async def main():
        """ä¸»ç¨‹åºå¼‚æ­¥å…¥å£å‡½æ•°
        
        è´Ÿè´£ç¨‹åºçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–ã€é¦–æ¬¡æ•°æ®æ›´æ–°ã€å®šæ—¶ä»»åŠ¡å¯åŠ¨å’Œä¼˜é›…é€€å‡ºã€‚
        è¿™æ˜¯æ•´ä¸ªçˆ¬è™«ç³»ç»Ÿçš„æ ¸å¿ƒæ§åˆ¶é€»è¾‘ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šå¯é åœ°è¿è¡Œã€‚
        
        Process:
            1. æ‰“å°å¯åŠ¨ä¿¡æ¯
            2. æ‰§è¡Œé¦–æ¬¡æ•°æ®æ›´æ–°ï¼ˆéªŒè¯ç³»ç»Ÿå¯ç”¨æ€§ï¼‰
            3. å¦‚æœé¦–æ¬¡æ›´æ–°æˆåŠŸï¼Œå¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
            4. å¤„ç†ç”¨æˆ·ä¸­æ–­ä¿¡å·ï¼Œä¼˜é›…åœæ­¢æœåŠ¡
            5. è®°å½•ç¨‹åºçŠ¶æ€å’Œå¼‚å¸¸ä¿¡æ¯
            
        Note:
            - é¦–æ¬¡æ›´æ–°å¤±è´¥ä¼šå¯¼è‡´ç¨‹åºé€€å‡ºï¼Œé¿å…æ— æ•ˆçš„å®šæ—¶ä»»åŠ¡
            - æ”¯æŒCtrl+Cä¼˜é›…ä¸­æ–­ï¼Œç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
            - æ‰€æœ‰å…³é”®æ“ä½œéƒ½æœ‰æ—¥å¿—è®°å½•
            - å®šæ—¶ä»»åŠ¡ä½¿ç”¨é…ç½®çš„åˆ·æ–°é—´éš”
            - å¼‚å¸¸æƒ…å†µä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä¸ä¼šå¯¼è‡´ç¨‹åºå´©æºƒ
        """
        print("è‚¡ç¥¨ç»„åˆæ’è¡Œæ¦œçˆ¬è™«å¯åŠ¨")
        
        # ç¨‹åºå¯åŠ¨æ—¶æ‰§è¡Œé¦–æ¬¡æ›´æ–°
        logger.info("ç¨‹åºå¯åŠ¨ï¼Œæ‰§è¡Œé¦–æ¬¡æ•°æ®æ›´æ–°")
        success = await crawler.run_update()
        
        if success:
            # å¯åŠ¨å®šæ—¶ä»»åŠ¡
            print(f"å®šæ—¶ä»»åŠ¡å¯åŠ¨ - å„æ¦œå•ï¼Œç»„åˆåˆ—è¡¨ï¼Œæ›´æ–°é—´éš”: {REFRESH_INTERVAL}ç§’")
            
            try:
                # ä½¿ç”¨é…ç½®çš„åˆ·æ–°é—´éš”
                crawler.setup_schedule(REFRESH_INTERVAL)
                crawler.start_scheduler()
            except KeyboardInterrupt:
                print("\næ­£åœ¨åœæ­¢...")
                crawler.stop_scheduler()
                print("ç¨‹åºå·²åœæ­¢")
        else:
            logger.error("é¦–æ¬¡æ•°æ®æ›´æ–°å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå¼‚å¸¸: {str(e)}")
    finally:
        logger.info("ç¨‹åºå·²é€€å‡º")